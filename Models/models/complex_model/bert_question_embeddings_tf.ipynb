{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_question_embeddings_tf.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C49OZoO3vtmT","executionInfo":{"status":"ok","timestamp":1618305597187,"user_tz":-120,"elapsed":43078,"user":{"displayName":"Andrea Grendene","photoUrl":"","userId":"16493131175776921557"}},"outputId":"32ea0143-81e2-403e-a025-a420981bc478"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rogbCXhJ-itn","executionInfo":{"status":"ok","timestamp":1618305621395,"user_tz":-120,"elapsed":67226,"user":{"displayName":"Andrea Grendene","photoUrl":"","userId":"16493131175776921557"}},"outputId":"8d367665-a431-47d5-ccfb-4a543ea9d1fb"},"source":["!pip install -q tensorflow-text\n","!pip install -q tf-models-official\n","!pip install tensorflow-determinism"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 3.4MB 11.1MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 12.1MB/s \n","\u001b[K     |████████████████████████████████| 645kB 41.9MB/s \n","\u001b[K     |████████████████████████████████| 37.6MB 78kB/s \n","\u001b[K     |████████████████████████████████| 358kB 37.9MB/s \n","\u001b[K     |████████████████████████████████| 706kB 41.1MB/s \n","\u001b[K     |████████████████████████████████| 1.2MB 36.5MB/s \n","\u001b[K     |████████████████████████████████| 174kB 44.9MB/s \n","\u001b[K     |████████████████████████████████| 102kB 10.3MB/s \n","\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorflow-determinism\n","  Downloading https://files.pythonhosted.org/packages/76/56/79d74f25b326d8719753172496abc524980fa67d1d98bb247021376e370a/tensorflow-determinism-0.3.0.tar.gz\n","Building wheels for collected packages: tensorflow-determinism\n","  Building wheel for tensorflow-determinism (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorflow-determinism: filename=tensorflow_determinism-0.3.0-cp37-none-any.whl size=9158 sha256=ffd19a48bf2ed13441d5776fc9a053779c55c3fc0bed13d919ae5a3ddc4c4ca8\n","  Stored in directory: /root/.cache/pip/wheels/66/c3/18/13959a90d3e0d10182a99866d6ff4d0119e9daed6ce014b54c\n","Successfully built tensorflow-determinism\n","Installing collected packages: tensorflow-determinism\n","Successfully installed tensorflow-determinism-0.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hMIbBuPy-wed","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618337714470,"user_tz":-120,"elapsed":480478,"user":{"displayName":"Andrea Grendene","photoUrl":"","userId":"16493131175776921557"}},"outputId":"7026dbc9-d0a4-4b46-f024-584e67fe4534"},"source":["general_settings = {\n","    \"seed\": 2021,\n","    \"batch_size\": 32,\n","    \"validation_split\": 0.15,\n","    \"destination_path\": \"/content/gdrive/MyDrive/Colab Notebooks/output\",\n","    \"threshold\": 0.55,\n","    \"embedding_size\": 0,\n","    \"lstm_hidden_size\": 128\n","}\n","\n","import os\n","from typing import List, Dict, Any, Tuple\n","import json\n","import shutil\n","from sklearn.metrics import accuracy_score, classification_report\n","import random\n","import numpy as np\n","\n","\"\"\"# Set seed to prevent non-determinism\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","os.environ['TF_CUDNN_DETERMINISTIC']='1'\n","os.environ['PYTHONHASHSEED']=str(general_settings['seed'])\n","random.seed(general_settings['seed'])\n","np.random.seed(general_settings['seed'])\"\"\"\n","\n","import tensorflow as tf\n","tf.random.set_seed(general_settings['seed'])\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optmizer\n","from tensorflow.data import Dataset\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras import layers\n","\n","import matplotlib.pyplot as plt\n","\n","\"\"\"from fwd9m.tensorflow import enable_determinism\n","enable_determinism()\"\"\"\n","\n","tf.get_logger().setLevel('ERROR')\n","\n","\n","\n","def get_test_lists_from_elements(elements_list: List[Dict[str, Any]]) -> Tuple[List[str], List[int], tf.Tensor, tf.Tensor, tf.Tensor, List[int], List[int]]:\n","    questions = []\n","    labels = []\n","    templates_list = []\n","    embeddings_list = []\n","    num_embeddings_list = []\n","    # Save labels in a specific list, otherwise we cannot know the true label of questions without DeepPavlov candidate queries\n","    labels_list_per_question = []\n","    # Append every candidate query as an element to predict and save the number of candidate queries for each question. After model evaluation execution, for every\n","    # question the answer with the maximum probability will be the final prediction. Labels are saved for each candidate query for tensors size consistency, although\n","    # they will be used once per question\n","    for element in elements_list:\n","        num_embeddings_list.append(len(element['deeppavlov_embeddings']))\n","        labels_list_per_question.append(element['answerable'])\n","        for query_embeddings in element['deeppavlov_embeddings']:\n","            labels.append(element['answerable'])\n","            final_embedding = []\n","            for embedding in query_embeddings:\n","                final_embedding.extend(embedding)\n","            embeddings_list.append(final_embedding)\n","            templates_list.append(element['template_conv_encoding'])\n","            # If template encoding length is not saved yet into general_settings, calculate and save it. Do the same for embedding length if the saved value is smaller\n","            if not \"template_encoding_length\" in general_settings:\n","                general_settings['template_encoding_length'] = len(element['template_conv_encoding'])\n","            if general_settings['embedding_size'] < len(final_embedding):\n","                general_settings['embedding_size'] = len(final_embedding)\n","            if element['question']:\n","                questions.append(element['question'])\n","            else:\n","                questions.append(element['NNQT_question'])\n","    # Add temporarily a list with the maximum length of zeros, because the test set might not have any example with the maximum possible embeddings length, and so\n","    # the input dimension would be wrong. Doing so \"pad_sequence\" should pad train and test set uniformly. Before returning the embedding list the last element is dropped\n","    embeddings_list.append([0] * general_settings['embedding_size'])\n","    embeddings_list = tf.keras.preprocessing.sequence.pad_sequences(embeddings_list, padding=\"post\")\n","    embeddings_list = embeddings_list[:-1]\n","    embeddings_tensor = tf.convert_to_tensor(embeddings_list, dtype=tf.float32)\n","    masking_layer = layers.Masking()\n","    masked_embeddings_tensor = masking_layer(embeddings_tensor)\n","    templates_tensor = tf.convert_to_tensor(templates_list, dtype=tf.float32)\n","    return questions, labels, templates_tensor, embeddings_tensor, masked_embeddings_tensor, num_embeddings_list, labels_list_per_question\n","\n","# Support function to avoid code repetition, get questions and labels lists\n","def get_lists_from_elements(elements_list: List[Dict[str, Any]]) -> Tuple[List[str], List[int], tf.Tensor, tf.Tensor, tf.Tensor]:\n","    questions = []\n","    labels = []\n","    embeddings_list = []\n","    templates_list = []\n","    for element in elements_list:\n","        labels.append(element['answerable'])\n","        final_embedding = []\n","        for embedding in element['embeddings']:\n","            final_embedding.extend(embedding)\n","        embeddings_list.append(final_embedding)\n","        templates_list.append(element['template_encoding'])\n","        # If template encoding length is not saved yet into general_settings, calculate and save it. Do the same for embedding length if the saved value is smaller\n","        if not \"template_encoding_length\" in general_settings:\n","            general_settings['template_encoding_length'] = len(element['template_encoding'])\n","        if general_settings['embedding_size'] < len(final_embedding):\n","            general_settings['embedding_size'] = len(final_embedding)\n","        if element['question']:\n","            questions.append(element['question'])\n","        else:\n","            questions.append(element['NNQT_question'])\n","    embeddings_list = tf.keras.preprocessing.sequence.pad_sequences(embeddings_list, padding=\"post\")\n","    embeddings_tensor = tf.convert_to_tensor(embeddings_list, dtype=tf.float32)\n","    masking_layer = layers.Masking()\n","    masked_embeddings_tensor = masking_layer(embeddings_tensor)\n","    templates_tensor = tf.convert_to_tensor(templates_list, dtype=tf.uint8)\n","    return questions, labels, templates_tensor, embeddings_tensor, masked_embeddings_tensor\n","\n","# Load dataset data\n","with open(\"/content/gdrive/MyDrive/Colab Notebooks/data/LC_QuAD_2_train_balanced_with_embeddings_no_dp.json\", \"r\") as json_file:\n","    train_data = json.load(json_file)\n","    train_questions, train_labels, train_templates, train_embeddings, train_masked_embeddings = get_lists_from_elements(train_data)\n","with open(\"/content/gdrive/MyDrive/Colab Notebooks/data/LC_QuAD_2_valid_balanced_with_embeddings_no_dp.json\", \"r\") as json_file:\n","    valid_data = json.load(json_file)\n","    valid_questions, valid_labels, valid_templates, valid_embeddings, valid_masked_embeddings = get_lists_from_elements(valid_data)\n","with open(\"/content/gdrive/MyDrive/Colab Notebooks/data/LC_QuAD_2_test_balanced_with_embeddings.json\", \"r\") as json_file:\n","    test_data = json.load(json_file)\n","    test_questions, test_labels, test_templates, test_embeddings, test_masked_embeddings, \\\n","    test_num_embeddings_list, test_labels_list_per_question = get_test_lists_from_elements(test_data)\n","\n","# Create datasets\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","\"\"\"train_set_length = len(train_questions)\n","val_set_length = int(train_set_length * general_settings['validation_split'])\"\"\"\n","train_set = Dataset.from_tensor_slices(({\"questions\": train_questions, \"templates\": train_templates, \"embeddings\": train_embeddings, \\\n","                                         \"masked_embeddings\": train_masked_embeddings}, train_labels))\n","\n","val_set = Dataset.from_tensor_slices(({\"questions\": valid_questions, \"templates\": valid_templates, \"embeddings\": valid_embeddings, \\\n","                                       \"masked_embeddings\": valid_masked_embeddings}, valid_labels))\n","\n","\"\"\"train_set = train_set.shuffle(train_set_length, seed=general_settings['seed'])\n","val_set = train_set.take(val_set_length)\n","train_set = train_set.skip(val_set_length)\"\"\"\n","train_set = train_set.batch(general_settings['batch_size'])\n","val_set = val_set.batch(general_settings['batch_size'])\n","train_set = train_set.cache().prefetch(buffer_size=AUTOTUNE)\n","val_set = val_set.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","test_set = Dataset.from_tensor_slices(({\"questions\": test_questions, \"templates\": test_templates, \"embeddings\": test_embeddings, \\\n","                                        \"masked_embeddings\": test_masked_embeddings}, test_labels))\n","test_set = test_set.batch(general_settings['batch_size'])\n","test_set = test_set.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","# Build model\n","tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n","tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n","\"\"\"tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4'\n","tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3'\"\"\"\n","\n","def build_classifier_model():\n","  questions_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='questions')\n","  templates_input = tf.keras.layers.Input(shape=(general_settings['template_encoding_length']), dtype=tf.float32, name='templates')\n","  embeddings_input = tf.keras.layers.Input(shape=(general_settings['embedding_size']), dtype=tf.float32, name='embeddings')\n","  masked_embeddings_input = tf.keras.layers.Input(shape=(general_settings['embedding_size']), dtype=tf.string, name='masked_embeddings')\n","  # Preprocess text input\n","  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","  encoder_inputs = preprocessing_layer(questions_input)\n","  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  # 'pooled_output' is the [CLS] token\n","  net = outputs['pooled_output']\n","  net = tf.keras.layers.Dropout(0.25)(net)\n","\n","  # Process embeddings\n","  embeddings_input_exp = tf.expand_dims(embeddings_input, axis=1)\n","  emb_net = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(general_settings['lstm_hidden_size']))(embeddings_input_exp)\n","  emb_net = tf.keras.layers.Dropout(0.25)(emb_net)\n","  emb_net = tf.keras.layers.Dense(general_settings['lstm_hidden_size'], activation='relu')(emb_net)\n","\n","  # Process the concatenation of template, BERT result and LSTM result\n","  net = tf.keras.layers.Dense(512 + general_settings['lstm_hidden_size'], activation=None)(tf.concat([templates_input, net, emb_net], 1))\n","  net = tf.keras.layers.Dense(512 + general_settings['lstm_hidden_size'], activation=None)(net)\n","  net = tf.keras.layers.Dense(512 + general_settings['lstm_hidden_size'], activation=None)(net)\n","  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n","  return tf.keras.Model([questions_input, templates_input, embeddings_input, masked_embeddings_input], net)\n","\n","# Define BCE loss function and accuracy metric\n","loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","metrics = tf.metrics.BinaryAccuracy(threshold=general_settings['threshold'])\n","\n","epochs = 70\n","steps_per_epoch = tf.data.experimental.cardinality(train_set).numpy()\n","num_train_steps = steps_per_epoch * epochs\n","# Number of steps (10%) of fixed learning rate before linear decay\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","# For BERT fine-tuning is recommended a low learning rate (between 2e-5 and 5e-5)\n","init_lr = 1e-6\n","# Optimizer is AdamW, a version of Adam that uses weights decay instead of moments\n","\"\"\"optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')\"\"\"\n","optimizer = Adam(init_lr)\n","# Build model and load information\n","classifier_model = build_classifier_model()\n","classifier_model.compile(optimizer=optimizer,\n","                         loss=loss,\n","                         metrics=metrics)\n","callbacks = [\n","    ModelCheckpoint(\n","        # Path where to save the model. The two parameters below mean that we will overwrite\n","        # the current checkpoint if and only if the `val_loss` score has improved.\n","        # The saved model name will include the current epoch.\n","        filepath=general_settings['destination_path'] + \"/bert_question_embeddings\",\n","        save_best_only=True,  # Only save a model if `val_loss` has improved.\n","        monitor=\"val_loss\",\n","        verbose=1,\n","    ),\n","  EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","]\n","\n","# Freeze BERT training\n","\"\"\"for w in classifier_model.get_layer('BERT_encoder').weights:\n","    w._trainable = False\"\"\"\n","\n","# Train\n","print(f'Training model with {tfhub_handle_encoder}')\n","\"\"\"history = classifier_model.fit(x=train_set,\n","                               validation_data=val_set,\n","                               shuffle=False,\n","                               epochs=epochs,\n","                               callbacks=callbacks)\"\"\"\n","\n","# Load model\n","classifier_model = load_model(general_settings['destination_path'] + \"/bert_question_embeddings\")\n","\n","# Test\n","y_pred = classifier_model.predict(test_set).tolist()\n","\n","# Get the label majority class\n","zero_labels = 0\n","one_labels = 0\n","for question_label in test_labels_list_per_question:\n","    if question_label == 0:\n","        zero_labels += 1\n","    else:\n","        one_labels += 1\n","# With a perfectly balanced test set, the chosen majority class is 0\n","if one_labels > zero_labels:\n","    majority_class = 1\n","else:\n","    majority_class = 0\n","# Save predictions for each question, choosing the candidate query with the max probability\n","real_y_pred = []\n","y_index = 0\n","for num_embeddings in test_num_embeddings_list:\n","    if num_embeddings > 0:\n","        for index in range(num_embeddings):\n","            if index == 0:\n","                best_output = y_pred[y_index]\n","                best_index = 0\n","            elif y_pred[y_index + index] > best_output:\n","                best_index = index\n","        real_y_pred.append(int(tf.sigmoid(y_pred[best_index]) > general_settings['threshold']))\n","        y_index += num_embeddings\n","    else:\n","        # Question without candidate queries, answer with the majority class\n","        real_y_pred.append(majority_class)\n","\n","# Metrics print\n","print('Classification Report:')\n","print(classification_report(test_labels_list_per_question, real_y_pred, labels=[1,0], digits=4))\n","\n","# Write predictions to file\n","with open(general_settings['destination_path'] + \"/bert_question_embeddings/model_predictions.txt\", \"w\") as answers_file:\n","    answers_file.write(str(real_y_pred))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <function CapturableResourceDeleter.__del__ at 0x7fefa3f0f560>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py\", line 208, in __del__\n","    self._destroy_resource()\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n","    result = self._call(*args, **kwds)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n","    self._initialize(args, kwds, add_initializers_to=initializers)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n","    *args, **kwds))\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n","    graph_function, _ = self._maybe_define_function(args, kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n","    graph_function = self._create_graph_function(args, kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n","    capture_by_value=self._capture_by_value),\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n","    func_outputs = python_func(*func_args, **func_kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n","    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 253, in restored_function_body\n","    return _call_concrete_function(function, inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 75, in _call_concrete_function\n","    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\", line 116, in _call_flat\n","    cancellation_manager)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1932, in _call_flat\n","    flat_outputs = forward_function.call(ctx, args_with_tangents)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 589, in call\n","    executor_type=executor_type)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/functional_ops.py\", line 1206, in partitioned_call\n","    f.add_to_graph(graph)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 505, in add_to_graph\n","    g._add_function(self)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3396, in _add_function\n","    gradient)\n","tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\n"],"name":"stderr"},{"output_type":"stream","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","           1     0.5529    0.6552    0.5997       319\n","           0     0.5436    0.4367    0.4843       300\n","\n","    accuracy                         0.5493       619\n","   macro avg     0.5482    0.5459    0.5420       619\n","weighted avg     0.5484    0.5493    0.5438       619\n","\n"],"name":"stdout"}]}]}