{
  "without english answers": {
    "not found": 0,
    "wrong answers": 0,
    "right answers paper": 0,
    "wrong answers paper": 0,
    "right prediction wrong answer number": 0,
    "wrong prediction right answer number": 0,
    "total number": 0,
    "answers": []
  },
  "total": {
    "right answers answerable": 75,
    "not found answerable": 102,
    "wrong answers answerable": 142,
    "right answers paper answerable": 144,
    "wrong answers paper answerable": 175,
    "right prediction wrong answer number answerable": 53,
    "wrong prediction right answer number answerable": 38,
    "total number answerable": 319,
    "operation types statistics": {
      "entity 1 not found": 21,
      "entity 1 wrong answers": 34,
      "entity 1 right answers paper": 8,
      "entity 1 wrong answers paper": 47,
      "entity 1 right prediction wrong answer number": 26,
      "entity 1 wrong prediction right answer number": 4,
      "entity 1 total number": 55,
      "relation 1 not found": 13,
      "relation 1 wrong answers": 27,
      "relation 1 right answers paper": 15,
      "relation 1 wrong answers paper": 25,
      "relation 1 right prediction wrong answer number": 22,
      "relation 1 wrong prediction right answer number": 1,
      "relation 1 total number": 40,
      "entity 2 not found": 23,
      "entity 2 wrong answers": 28,
      "entity 2 right answers paper": 12,
      "entity 2 wrong answers paper": 39,
      "entity 2 right prediction wrong answer number": 13,
      "entity 2 wrong prediction right answer number": 9,
      "entity 2 total number": 51,
      "relation 2 not found": 17,
      "relation 2 wrong answers": 33,
      "relation 2 right answers paper": 19,
      "relation 2 wrong answers paper": 31,
      "relation 2 right prediction wrong answer number": 23,
      "relation 2 wrong prediction right answer number": 6,
      "relation 2 total number": 50,
      "entity 3 not found": 19,
      "entity 3 wrong answers": 33,
      "entity 3 right answers paper": 15,
      "entity 3 wrong answers paper": 37,
      "entity 3 right prediction wrong answer number": 23,
      "entity 3 wrong prediction right answer number": 7,
      "entity 3 total number": 52,
      "relation 3 not found": 12,
      "relation 3 wrong answers": 40,
      "relation 3 right answers paper": 21,
      "relation 3 wrong answers paper": 31,
      "relation 3 right prediction wrong answer number": 24,
      "relation 3 wrong prediction right answer number": 4,
      "relation 3 total number": 52,
      "accuracy entity 1": 0.38181818181818183,
      "accuracy paper entity 1": 0.14545454545454545,
      "accuracy with predictions entity 1": 0.7818181818181819,
      "accuracy relation 1": 0.325,
      "accuracy paper relation 1": 0.375,
      "accuracy with predictions relation 1": 0.85,
      "accuracy entity 2": 0.45098039215686275,
      "accuracy paper entity 2": 0.23529411764705882,
      "accuracy with predictions entity 2": 0.5294117647058824,
      "accuracy relation 2": 0.34,
      "accuracy paper relation 2": 0.38,
      "accuracy with predictions relation 2": 0.68,
      "accuracy entity 3": 0.36538461538461536,
      "accuracy paper entity 3": 0.28846153846153844,
      "accuracy with predictions entity 3": 0.6730769230769231,
      "accuracy relation 3": 0.23076923076923078,
      "accuracy paper relation 3": 0.40384615384615385,
      "accuracy with predictions relation 3": 0.6153846153846154
    },
    "not found not answerable": 105,
    "wrong answers not answerable": 195,
    "right answers paper not answerable": 90,
    "wrong answers paper not answerable": 210,
    "right prediction wrong answer number not answerable": 131,
    "wrong prediction right answer number not answerable": 31,
    "total number not answerable": 300,
    "total number": 619,
    "accuracy answerable": 0.23510971786833856,
    "accuracy paper answerable": 0.45141065830721006,
    "accuracy with predictions answerable": 0.28213166144200624,
    "accuracy not answerable": 0.35,
    "accuracy paper not answerable": 0.3,
    "accuracy with predictions not answerable": 0.6833333333333333,
    "final accuracy": 0.29079159935379645,
    "final accuracy paper": 0.3780290791599354,
    "final accuracy with answerable predictions": 0.3150242326332795,
    "final accuracy with not answerable predictions": 0.45234248788368336,
    "final accuracy with all predictions": 0.4765751211631664,
    "final accuracy answerable predictions no I don't know": 0.2508833922261484,
    "final accuracy all predictions no I don't know": 0.4275618374558304
  },
  "rank": {
    "right answers answerable": 0,
    "not found answerable": 6,
    "wrong answers answerable": 0,
    "right answers paper answerable": 0,
    "wrong answers paper answerable": 6,
    "right prediction wrong answer number answerable": 6,
    "wrong prediction right answer number answerable": 0,
    "total number answerable": 6,
    "operation types statistics": {
      "entity 1 not found": 1,
      "entity 1 wrong answers": 0,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 1,
      "entity 1 right prediction wrong answer number": 0,
      "entity 1 wrong prediction right answer number": 1,
      "entity 1 total number": 1,
      "relation 1 not found": 0,
      "relation 1 wrong answers": 1,
      "relation 1 right answers paper": 0,
      "relation 1 wrong answers paper": 1,
      "relation 1 right prediction wrong answer number": 0,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 1,
      "entity 2 not found": 0,
      "entity 2 wrong answers": 0,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 0,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 0,
      "relation 2 not found": 1,
      "relation 2 wrong answers": 0,
      "relation 2 right answers paper": 0,
      "relation 2 wrong answers paper": 1,
      "relation 2 right prediction wrong answer number": 0,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 1,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 0,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 1,
      "entity 3 right prediction wrong answer number": 0,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 1,
      "relation 3 not found": 0,
      "relation 3 wrong answers": 1,
      "relation 3 right answers paper": 0,
      "relation 3 wrong answers paper": 1,
      "relation 3 right prediction wrong answer number": 1,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 1,
      "accuracy entity 1": 1.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 0.0,
      "accuracy relation 1": 0.0,
      "accuracy paper relation 1": 0.0,
      "accuracy with predictions relation 1": 0.0,
      "accuracy entity 2": 0,
      "accuracy paper entity 2": 0,
      "accuracy with predictions entity 2": 0,
      "accuracy relation 2": 1.0,
      "accuracy paper relation 2": 0.0,
      "accuracy with predictions relation 2": 1.0,
      "accuracy entity 3": 1.0,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 0.0,
      "accuracy paper relation 3": 0.0,
      "accuracy with predictions relation 3": 1.0
    },
    "not found not answerable": 3,
    "wrong answers not answerable": 2,
    "right answers paper not answerable": 0,
    "wrong answers paper not answerable": 5,
    "right prediction wrong answer number not answerable": 1,
    "wrong prediction right answer number not answerable": 1,
    "total number not answerable": 5,
    "total number": 11,
    "answers": [
      {
        "question index": 0,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "nonane",
          "Dicyclopentadiene",
          "turpentine",
          "tetraethyl silicate",
          "o-chlorotoluene"
        ]
      },
      {
        "question index": 70,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 98,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 316,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 352,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "South Korea",
          "South Korea",
          "Israel",
          "Australia"
        ]
      },
      {
        "question index": 354,
        "answer": "1",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 382,
        "answer": "foreign relations of the European Union",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 449,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Cisco Systems",
          "Cisco Systems",
          "Cisco Systems",
          "Cisco Systems",
          "Cisco Systems"
        ]
      },
      {
        "question index": 545,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "petroleum"
        ]
      },
      {
        "question index": 554,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "NGC 281",
          "Messier 37",
          "Messier 26",
          "Messier 46",
          "Alpha Persei Cluster"
        ]
      },
      {
        "question index": 650,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "2020 Beirut explosions",
          "Tlahuelilpan pipeline explosion",
          "JSC “GosNII “Kristall” explosion",
          "Deepwater Horizon explosion",
          "2018 Magnitogorsk building collapse"
        ]
      }
    ],
    "accuracy answerable": 0.0,
    "accuracy paper answerable": 0.0,
    "accuracy with predictions answerable": 1.0,
    "accuracy not answerable": 0.6,
    "accuracy paper not answerable": 0.0,
    "accuracy with predictions not answerable": 0.6,
    "final accuracy": 0.2727272727272727,
    "final accuracy paper": 0.0,
    "final accuracy with answerable predictions": 0.8181818181818182,
    "final accuracy with not answerable predictions": 0.2727272727272727,
    "final accuracy with all predictions": 0.8181818181818182,
    "final accuracy answerable predictions no I don't know": 0.6,
    "final accuracy all predictions no I don't know": 0.6
  },
  "statement property": {
    "right answers answerable": 19,
    "not found answerable": 12,
    "wrong answers answerable": 8,
    "right answers paper answerable": 22,
    "wrong answers paper answerable": 17,
    "right prediction wrong answer number answerable": 5,
    "wrong prediction right answer number answerable": 9,
    "total number answerable": 39,
    "operation types statistics": {
      "entity 1 not found": 5,
      "entity 1 wrong answers": 2,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 7,
      "entity 1 right prediction wrong answer number": 2,
      "entity 1 wrong prediction right answer number": 1,
      "entity 1 total number": 7,
      "relation 1 not found": 1,
      "relation 1 wrong answers": 4,
      "relation 1 right answers paper": 1,
      "relation 1 wrong answers paper": 4,
      "relation 1 right prediction wrong answer number": 4,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 5,
      "entity 2 not found": 6,
      "entity 2 wrong answers": 1,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 7,
      "entity 2 right prediction wrong answer number": 1,
      "entity 2 wrong prediction right answer number": 2,
      "entity 2 total number": 7,
      "relation 2 not found": 3,
      "relation 2 wrong answers": 4,
      "relation 2 right answers paper": 3,
      "relation 2 wrong answers paper": 4,
      "relation 2 right prediction wrong answer number": 3,
      "relation 2 wrong prediction right answer number": 1,
      "relation 2 total number": 7,
      "entity 3 not found": 4,
      "entity 3 wrong answers": 2,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 6,
      "entity 3 right prediction wrong answer number": 1,
      "entity 3 wrong prediction right answer number": 1,
      "entity 3 total number": 6,
      "relation 3 not found": 1,
      "relation 3 wrong answers": 6,
      "relation 3 right answers paper": 4,
      "relation 3 wrong answers paper": 3,
      "relation 3 right prediction wrong answer number": 5,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 7,
      "accuracy entity 1": 0.7142857142857143,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 0.8571428571428571,
      "accuracy relation 1": 0.2,
      "accuracy paper relation 1": 0.2,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 0.8571428571428571,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 0.7142857142857143,
      "accuracy relation 2": 0.42857142857142855,
      "accuracy paper relation 2": 0.42857142857142855,
      "accuracy with predictions relation 2": 0.7142857142857143,
      "accuracy entity 3": 0.6666666666666666,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 0.6666666666666666,
      "accuracy relation 3": 0.14285714285714285,
      "accuracy paper relation 3": 0.5714285714285714,
      "accuracy with predictions relation 3": 0.8571428571428571
    },
    "not found not answerable": 20,
    "wrong answers not answerable": 19,
    "right answers paper not answerable": 8,
    "wrong answers paper not answerable": 31,
    "right prediction wrong answer number not answerable": 16,
    "wrong prediction right answer number not answerable": 5,
    "total number not answerable": 39,
    "total number": 78,
    "answers": [
      {
        "question index": 1,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 10,
        "answer": "Tony Award for Best Actress in a Musical",
        "prediction": "False",
        "true answers": [
          "Tony Award for Best Actress in a Musical"
        ]
      },
      {
        "question index": 17,
        "answer": "Jagdfliegergeschwader 8",
        "prediction": "False",
        "true answers": [
          "Jagdfliegergeschwader 8"
        ]
      },
      {
        "question index": 29,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "United States senator"
        ]
      },
      {
        "question index": 30,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 31,
        "answer": "9380854",
        "prediction": "True",
        "true answers": [
          "9380854"
        ]
      },
      {
        "question index": 32,
        "answer": "Academy Award for Best Supporting Actress",
        "prediction": "False",
        "true answers": [
          "Academy Award for Best Supporting Actress"
        ]
      },
      {
        "question index": 50,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 52,
        "answer": "Baghdad",
        "prediction": "False",
        "true answers": [
          "historical country"
        ]
      },
      {
        "question index": 54,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "1559155"
        ]
      },
      {
        "question index": 57,
        "answer": "Mary Ure",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 59,
        "answer": "Berlin",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 60,
        "answer": "27469114",
        "prediction": "True",
        "true answers": [
          "Rick Perry"
        ]
      },
      {
        "question index": 61,
        "answer": "Palazzo Barberini",
        "prediction": "True",
        "true answers": [
          "Gian"
        ]
      },
      {
        "question index": 75,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "United States senator"
        ]
      },
      {
        "question index": 76,
        "answer": "United States Senator",
        "prediction": "False",
        "true answers": [
          "United States Secretary of State"
        ]
      },
      {
        "question index": 80,
        "answer": "7587800",
        "prediction": "True",
        "true answers": [
          "7587800"
        ]
      },
      {
        "question index": 88,
        "answer": "Mario Oliverio",
        "prediction": "False",
        "true answers": [
          "Giuseppe Scopelliti"
        ]
      },
      {
        "question index": 110,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Peabody Award"
        ]
      },
      {
        "question index": 140,
        "answer": "Minister President of Prussia",
        "prediction": "True",
        "true answers": [
          "Vice-Chancellor of Germany",
          "Minister President of Prussia"
        ]
      },
      {
        "question index": 150,
        "answer": "United States representative",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 152,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 153,
        "answer": "466-01-01T00:00:00Z",
        "prediction": "True",
        "true answers": [
          "0180-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 161,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Cs-m%C3%A1slo.ogg"
        ]
      },
      {
        "question index": 165,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 181,
        "answer": "59000",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 183,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 185,
        "answer": "UTC+04:00",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 193,
        "answer": "Ariel Award for Best Actress",
        "prediction": "False",
        "true answers": [
          "Ariel Award for Best Actress"
        ]
      },
      {
        "question index": 209,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 215,
        "answer": "141463",
        "prediction": "True",
        "true answers": [
          "141463"
        ]
      },
      {
        "question index": 217,
        "answer": "Daddy Gaoh",
        "prediction": "False",
        "true answers": [
          "Daddy Gaoh"
        ]
      },
      {
        "question index": 226,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "court painter"
        ]
      },
      {
        "question index": 232,
        "answer": "Order of St Patrick",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 245,
        "answer": "Toni Nadal",
        "prediction": "True",
        "true answers": [
          "Toni Nadal"
        ]
      },
      {
        "question index": 247,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 256,
        "answer": "59000",
        "prediction": "False",
        "true answers": [
          "59000"
        ]
      },
      {
        "question index": 257,
        "answer": "Mary Ure",
        "prediction": "True",
        "true answers": [
          "Mary Ure"
        ]
      },
      {
        "question index": 258,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 263,
        "answer": "Khabarovsk Krai",
        "prediction": "True",
        "true answers": [
          "Khabarovsk Urban Okrug"
        ]
      },
      {
        "question index": 292,
        "answer": "Academy Award for Best Original Dramatic Score",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 293,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Heinrich"
        ]
      },
      {
        "question index": 313,
        "answer": "Academy Award for Best Original Dramatic or Comedy Score",
        "prediction": "True",
        "true answers": [
          "Academy Award for Best Original Dramatic or Comedy Score"
        ]
      },
      {
        "question index": 320,
        "answer": "8684",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 326,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 330,
        "answer": "Mario Oliverio",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 331,
        "answer": "Tony Award for Best Actress in a Musical",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 346,
        "answer": "100-01-01T00:00:00Z",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 347,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 371,
        "answer": "Primetime Emmy Award for Outstanding Music Composition for a Series",
        "prediction": "True",
        "true answers": [
          "Primetime Emmy Award for Outstanding Music Composition for a Series",
          "Primetime Emmy Award for Outstanding Animated Program",
          "Satellite Award for Best Television Series – Musical or Comedy",
          "Kids' Choice Award for Favorite Cartoon"
        ]
      },
      {
        "question index": 421,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Academy Award for Best Supporting Actor"
        ]
      },
      {
        "question index": 461,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Rosh HaAyin",
          "Batumi",
          "Klaipėda"
        ]
      },
      {
        "question index": 469,
        "answer": "member of the Chamber of Deputies of Chile",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 476,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 478,
        "answer": "Deputy President of South Africa",
        "prediction": "False",
        "true answers": [
          "Deputy President of South Africa"
        ]
      },
      {
        "question index": 479,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Austria-Hungary",
          "Cisleithania"
        ]
      },
      {
        "question index": 480,
        "answer": "1209091",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 485,
        "answer": "former country in Europe",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 490,
        "answer": "Lyon OU",
        "prediction": "False",
        "true answers": [
          "Lyon OU"
        ]
      },
      {
        "question index": 499,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 500,
        "answer": "University of Padua",
        "prediction": "True",
        "true answers": [
          "University of Padua",
          "University of Ferrara"
        ]
      },
      {
        "question index": 513,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 525,
        "answer": "Reuven Rivlin",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 533,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "United States of America"
        ]
      },
      {
        "question index": 538,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 544,
        "answer": "Primetime Emmy Award for Outstanding Music Composition for a Series",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 549,
        "answer": "politician",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 584,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "1729993",
          "2602460",
          "1406382",
          "1702508",
          "2647660",
          "1993403",
          "2586574",
          "2250087",
          "1739084",
          "1935161",
          "1287147",
          "2644391",
          "2527330",
          "1552832",
          "2610353",
          "2629592",
          "2424856",
          "1832934",
          "2102808",
          "2636092"
        ]
      },
      {
        "question index": 591,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 605,
        "answer": "27469114",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 608,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 626,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 629,
        "answer": "Daddy Gaoh",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 632,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 642,
        "answer": "Grammy Award for Best Classical Album",
        "prediction": "False",
        "true answers": [
          "Grammy Award for Best Classical Album",
          "Grammy Award for Best Instrumental Soloist Performance (without orchestra)"
        ]
      },
      {
        "question index": 643,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 649,
        "answer": "Stanley Edgar Hyman",
        "prediction": "True",
        "true answers": [
          "Stanley Edgar Hyman"
        ]
      },
      {
        "question index": 683,
        "answer": "Point(55.448055555556 -20.878888888889)",
        "prediction": "True",
        "true answers": [
          "UTC+02:00"
        ]
      }
    ],
    "accuracy answerable": 0.48717948717948717,
    "accuracy paper answerable": 0.5641025641025641,
    "accuracy with predictions answerable": 0.38461538461538464,
    "accuracy not answerable": 0.5128205128205128,
    "accuracy paper not answerable": 0.20512820512820512,
    "accuracy with predictions not answerable": 0.7948717948717948,
    "final accuracy": 0.5,
    "final accuracy paper": 0.38461538461538464,
    "final accuracy with answerable predictions": 0.44871794871794873,
    "final accuracy with not answerable predictions": 0.6410256410256411,
    "final accuracy with all predictions": 0.5897435897435898,
    "final accuracy answerable predictions no I don't know": 0.410958904109589,
    "final accuracy all predictions no I don't know": 0.5616438356164384
  },
  "string matching simple contains word": {
    "right answers answerable": 0,
    "not found answerable": 6,
    "wrong answers answerable": 18,
    "right answers paper answerable": 0,
    "wrong answers paper answerable": 24,
    "right prediction wrong answer number answerable": 5,
    "wrong prediction right answer number answerable": 0,
    "total number answerable": 24,
    "operation types statistics": {
      "entity 1 not found": 0,
      "entity 1 wrong answers": 3,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 3,
      "entity 1 right prediction wrong answer number": 3,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 3,
      "relation 1 not found": 2,
      "relation 1 wrong answers": 1,
      "relation 1 right answers paper": 0,
      "relation 1 wrong answers paper": 3,
      "relation 1 right prediction wrong answer number": 1,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 3,
      "entity 2 not found": 2,
      "entity 2 wrong answers": 2,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 4,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 1,
      "entity 2 total number": 4,
      "relation 2 not found": 0,
      "relation 2 wrong answers": 3,
      "relation 2 right answers paper": 0,
      "relation 2 wrong answers paper": 3,
      "relation 2 right prediction wrong answer number": 2,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 3,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 3,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 4,
      "entity 3 right prediction wrong answer number": 1,
      "entity 3 wrong prediction right answer number": 1,
      "entity 3 total number": 4,
      "relation 3 not found": 0,
      "relation 3 wrong answers": 4,
      "relation 3 right answers paper": 0,
      "relation 3 wrong answers paper": 4,
      "relation 3 right prediction wrong answer number": 1,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 4,
      "accuracy entity 1": 0.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 1.0,
      "accuracy relation 1": 0.6666666666666666,
      "accuracy paper relation 1": 0.0,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 0.5,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 0.25,
      "accuracy relation 2": 0.0,
      "accuracy paper relation 2": 0.0,
      "accuracy with predictions relation 2": 0.6666666666666666,
      "accuracy entity 3": 0.25,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 0.25,
      "accuracy relation 3": 0.0,
      "accuracy paper relation 3": 0.0,
      "accuracy with predictions relation 3": 0.25
    },
    "not found not answerable": 5,
    "wrong answers not answerable": 16,
    "right answers paper not answerable": 0,
    "wrong answers paper not answerable": 21,
    "right prediction wrong answer number not answerable": 8,
    "wrong prediction right answer number not answerable": 2,
    "total number not answerable": 21,
    "total number": 45,
    "answers": [
      {
        "question index": 2,
        "answer": "tangible good",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 12,
        "answer": "Otto's encyclopedia",
        "prediction": "True",
        "true answers": [
          "Visitation"
        ]
      },
      {
        "question index": 23,
        "answer": "Internet art",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 34,
        "answer": "Byzantine emperor",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 64,
        "answer": "marble",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 74,
        "answer": "radiation",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 85,
        "answer": "Fourth Letter",
        "prediction": "True",
        "true answers": [
          "zucchini",
          "Zitronengelb"
        ]
      },
      {
        "question index": 97,
        "answer": "Bardo",
        "prediction": "True",
        "true answers": [
          "Chilean transition to democracy",
          "transition"
        ]
      },
      {
        "question index": 99,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 100,
        "answer": "1598-01-01T00:00:00Z",
        "prediction": "False",
        "true answers": [
          "Paduspanids"
        ]
      },
      {
        "question index": 102,
        "answer": "Matteo",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 112,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Uranus"
        ]
      },
      {
        "question index": 145,
        "answer": "carbon",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 151,
        "answer": "Thyrididae",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 201,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 210,
        "answer": "Ursa Major",
        "prediction": "True",
        "true answers": [
          "UGC 12892",
          "UGC 12897",
          "UGC 12904",
          "UGC 12905",
          "UGC 12895",
          "UGC 12901",
          "UGC 12912",
          "UGC 12900",
          "UGC 12913",
          "UGC 12903",
          "UGC 12915",
          "UGC 12898",
          "UGC 12910",
          "UGC 12916",
          "UGC 1813",
          "UGC 4",
          "UGC 3521",
          "UGC 2885",
          "UGC 1382",
          "UGC 3522",
          "UGC 3336",
          "UGC 3500",
          "UGC 3442",
          "UGC 3528",
          "UGC 4078"
        ]
      },
      {
        "question index": 211,
        "answer": "synthpop",
        "prediction": "False",
        "true answers": [
          "vignetting"
        ]
      },
      {
        "question index": 235,
        "answer": "Unmasked",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 240,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Zauberturm",
          "Zirè",
          "Zatch Bell! The Card Battle"
        ]
      },
      {
        "question index": 253,
        "answer": "2011-01-01T00:00:00Z",
        "prediction": "True",
        "true answers": [
          "T-type brown dwarf"
        ]
      },
      {
        "question index": 271,
        "answer": "English",
        "prediction": "True",
        "true answers": [
          "tilt"
        ]
      },
      {
        "question index": 281,
        "answer": "ionizing radiation",
        "prediction": "True",
        "true answers": [
          "neutron radiation"
        ]
      },
      {
        "question index": 286,
        "answer": "Yarra Trams",
        "prediction": "True",
        "true answers": [
          "vehicle",
          "motor vehicle",
          "land vehicle",
          "off-road vehicle",
          "electric vehicle"
        ]
      },
      {
        "question index": 308,
        "answer": "Fourth Letter",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 319,
        "answer": "Second Letter",
        "prediction": "True",
        "true answers": [
          "weight",
          "Water resistance",
          "Wigner–Seitz radius"
        ]
      },
      {
        "question index": 342,
        "answer": "male",
        "prediction": "True",
        "true answers": [
          "Young Girls in Black",
          "Young Virgin Auto-Sodomized by the Horns of Her Own Chastity",
          "A Young Tiger Playing with its Mother",
          "Three Angels and Young Tobias",
          "Portrait of a young woman",
          "Portrait of a Young Man",
          "Portrait of a Young Woman",
          "Allegory of Salvation with the Virgin and Christ Child, St. Elizabeth, the Young St. John the Baptist and Two Angels",
          "A Young Girl Reading",
          "Young man in Mayo costume",
          "Young Spartans Exercising",
          "Portrait of a Young Woman",
          "Young Lady in 1866",
          "Portrait of a Young Man",
          "Portrait of a Young Man",
          "Portrait of a Young Man",
          "Portrait of a Young Man in Black",
          "Portrait of a Young Man in Red",
          "Portrait of a Young Man with a Book",
          "Portrait of a Young Man Seated on a Carpet",
          "Portrait of a Young Woman (Rosso Fiorentino)",
          "Portrait of a Young Man",
          "Portrait of a Young Man",
          "Portrait of a Young Man",
          "Portrait of a Young Man"
        ]
      },
      {
        "question index": 344,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Tall Tale",
          "Pirates of the Caribbean: Dead Men Tell No Tales",
          "Squanto: A Warrior's Tale",
          "Whispers: An Elephant's Tale"
        ]
      },
      {
        "question index": 396,
        "answer": "journalist",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 402,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Zrzez"
        ]
      },
      {
        "question index": 412,
        "answer": "fluoride ion",
        "prediction": "True",
        "true answers": [
          "iodide ion"
        ]
      },
      {
        "question index": 420,
        "answer": "1941-04-06T00:00:00Z",
        "prediction": "False",
        "true answers": [
          "American Revolutionary War",
          "Venezuelan War of Independence",
          "Peninsular War",
          "Mexican War of Independence",
          "Greek War of Independence",
          "Irish War of Independence",
          "Algerian War of Independence",
          "Second Italian War of Independence",
          "Turkish War of Independence",
          "Wars of Scottish Independence",
          "Spanish American wars of independence",
          "Namibian War of Independence",
          "Eritrean War for Liberation",
          "Serbo-Turkish War (1876–78)",
          "Latin American wars of independence",
          "Guinea-Bissau War of Independence",
          "War of Independence of Brazil",
          "Italian liberation war",
          "Mozambican War of Independence",
          "Angolan War of Independence"
        ]
      },
      {
        "question index": 424,
        "answer": "Kanashimi Johnny",
        "prediction": "True",
        "true answers": [
          "Teh tarik"
        ]
      },
      {
        "question index": 441,
        "answer": "English",
        "prediction": "True",
        "true answers": [
          "Kagoshima Space Center",
          "Goddard Space Flight Center",
          "Kennedy Space Center",
          "Vikram Sarabhai Space Centre",
          "Uchinoura Space Center",
          "Manned Space Flight Network",
          "Tanegashima Space Center",
          "Teófilo Tabanera Space Center"
        ]
      },
      {
        "question index": 448,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 451,
        "answer": "Ursa Major",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 473,
        "answer": "1941-04-06T00:00:00Z",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 475,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 505,
        "answer": "Second Letter",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 522,
        "answer": "drama film",
        "prediction": "False",
        "true answers": [
          "seal cutting",
          "sculpture"
        ]
      },
      {
        "question index": 528,
        "answer": "pornographic actor",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 535,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "VGF",
          "Vgf",
          "Vgf",
          "VGF",
          "vgf"
        ]
      },
      {
        "question index": 540,
        "answer": "event",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 547,
        "answer": "Armenian Soviet Encyclopedia",
        "prediction": "False",
        "true answers": [
          "volkerpsychologie"
        ]
      },
      {
        "question index": 558,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 670,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Dionysus"
        ]
      }
    ],
    "accuracy answerable": 0.0,
    "accuracy paper answerable": 0.0,
    "accuracy with predictions answerable": 0.20833333333333334,
    "accuracy not answerable": 0.23809523809523808,
    "accuracy paper not answerable": 0.0,
    "accuracy with predictions not answerable": 0.5238095238095238,
    "final accuracy": 0.1111111111111111,
    "final accuracy paper": 0.0,
    "final accuracy with answerable predictions": 0.2222222222222222,
    "final accuracy with not answerable predictions": 0.24444444444444444,
    "final accuracy with all predictions": 0.35555555555555557,
    "final accuracy answerable predictions no I don't know": 0.125,
    "final accuracy all predictions no I don't know": 0.275
  },
  "simple question right": {
    "right answers answerable": 10,
    "not found answerable": 1,
    "wrong answers answerable": 13,
    "right answers paper answerable": 11,
    "wrong answers paper answerable": 13,
    "right prediction wrong answer number answerable": 1,
    "wrong prediction right answer number answerable": 6,
    "total number answerable": 24,
    "operation types statistics": {
      "entity 1 not found": 0,
      "entity 1 wrong answers": 4,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 4,
      "entity 1 right prediction wrong answer number": 2,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 4,
      "relation 1 not found": 0,
      "relation 1 wrong answers": 3,
      "relation 1 right answers paper": 1,
      "relation 1 wrong answers paper": 2,
      "relation 1 right prediction wrong answer number": 3,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 3,
      "entity 2 not found": 0,
      "entity 2 wrong answers": 3,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 3,
      "entity 2 right prediction wrong answer number": 1,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 3,
      "relation 2 not found": 2,
      "relation 2 wrong answers": 2,
      "relation 2 right answers paper": 2,
      "relation 2 wrong answers paper": 2,
      "relation 2 right prediction wrong answer number": 1,
      "relation 2 wrong prediction right answer number": 2,
      "relation 2 total number": 4,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 3,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 4,
      "entity 3 right prediction wrong answer number": 1,
      "entity 3 wrong prediction right answer number": 1,
      "entity 3 total number": 4,
      "relation 3 not found": 0,
      "relation 3 wrong answers": 4,
      "relation 3 right answers paper": 2,
      "relation 3 wrong answers paper": 2,
      "relation 3 right prediction wrong answer number": 2,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 4,
      "accuracy entity 1": 0.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 0.5,
      "accuracy relation 1": 0.0,
      "accuracy paper relation 1": 0.3333333333333333,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 0.0,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 0.3333333333333333,
      "accuracy relation 2": 0.5,
      "accuracy paper relation 2": 0.5,
      "accuracy with predictions relation 2": 0.25,
      "accuracy entity 3": 0.25,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 0.25,
      "accuracy relation 3": 0.0,
      "accuracy paper relation 3": 0.5,
      "accuracy with predictions relation 3": 0.5
    },
    "not found not answerable": 3,
    "wrong answers not answerable": 19,
    "right answers paper not answerable": 5,
    "wrong answers paper not answerable": 17,
    "right prediction wrong answer number not answerable": 10,
    "wrong prediction right answer number not answerable": 3,
    "total number not answerable": 22,
    "total number": 46,
    "answers": [
      {
        "question index": 3,
        "answer": "Thane district",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 8,
        "answer": "grave of Soong Ching-ling",
        "prediction": "True",
        "true answers": [
          "Sun Yat-sen Mausoleum"
        ]
      },
      {
        "question index": 14,
        "answer": "Taito Corporation",
        "prediction": "False",
        "true answers": [
          "Midway Games"
        ]
      },
      {
        "question index": 22,
        "answer": "Rembrandt House Museum",
        "prediction": "False",
        "true answers": [
          "Rembrandt House Museum"
        ]
      },
      {
        "question index": 25,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "New York City"
        ]
      },
      {
        "question index": 44,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 45,
        "answer": "Norwich",
        "prediction": "True",
        "true answers": [
          "Great Yarmouth",
          "King's Lynn and West Norfolk",
          "Norwich"
        ]
      },
      {
        "question index": 47,
        "answer": "Asia",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 48,
        "answer": "Asia",
        "prediction": "False",
        "true answers": [
          "Asia"
        ]
      },
      {
        "question index": 62,
        "answer": "Saint Petersburg",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 90,
        "answer": "Belgium",
        "prediction": "True",
        "true answers": [
          "Ireland",
          "Denmark",
          "Finland",
          "Italy",
          "Poland",
          "Austria",
          "Portugal",
          "Netherlands",
          "France",
          "Germany",
          "Latvia",
          "Slovakia",
          "Romania",
          "Bulgaria",
          "Croatia"
        ]
      },
      {
        "question index": 111,
        "answer": "Chinese characters",
        "prediction": "True",
        "true answers": [
          "legacy Hanzi"
        ]
      },
      {
        "question index": 120,
        "answer": "Paris",
        "prediction": "True",
        "true answers": [
          "Pasadena",
          "Glendale",
          "Torrance",
          "Inglewood",
          "Long Beach",
          "Burbank"
        ]
      },
      {
        "question index": 128,
        "answer": "Virginia",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 131,
        "answer": "MiG-21 fleet",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 154,
        "answer": "probucol",
        "prediction": "True",
        "true answers": [
          "clarithromycin",
          "pimozide",
          "azithromycin",
          "droperidol",
          "haloperidol",
          "chlorpromazine",
          "anagrelide",
          "dronedarone",
          "amiodarone",
          "quinidine",
          "sotalol",
          "pentamidine",
          "procainamide",
          "terfenadine",
          "sevoflurane",
          "flecainide",
          "sulpiride",
          "chloroquine",
          "astemizole",
          "escitalopram",
          "domperidone",
          "moxifloxacin",
          "disopyramide",
          "cisapride",
          "halofantrine",
          "sparfloxacin",
          "dofetilide",
          "ibutilide",
          "bepridil",
          "mesoridazine",
          "probucol",
          "arsenic trioxide",
          "cocaine",
          "thioridazine"
        ]
      },
      {
        "question index": 155,
        "answer": "beverage industry",
        "prediction": "True",
        "true answers": [
          "The Coca-Cola Company"
        ]
      },
      {
        "question index": 168,
        "answer": "Stephen Adly Guirgis",
        "prediction": "True",
        "true answers": [
          "Kiss of Judas"
        ]
      },
      {
        "question index": 173,
        "answer": "Chinese characters",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 188,
        "answer": "International Football Association Board",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 241,
        "answer": "President of the Philippines",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 243,
        "answer": "Vostok 1",
        "prediction": "True",
        "true answers": [
          "Vostok 1"
        ]
      },
      {
        "question index": 244,
        "answer": "Stefan Pichler",
        "prediction": "True",
        "true answers": [
          "Al Jazeera Media Network"
        ]
      },
      {
        "question index": 248,
        "answer": "Stadtarchiv und Stadthistorische Bibliothek Bonn",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 273,
        "answer": "Chief Justice of the United States",
        "prediction": "False",
        "true answers": [
          "Chief Justice of the United States",
          "United States Secretary of State"
        ]
      },
      {
        "question index": 288,
        "answer": "Mercury-Redstone 3",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 290,
        "answer": "beverage industry",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 296,
        "answer": "Shenzhen",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 357,
        "answer": "United States of America",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 362,
        "answer": "Stadtarchiv und Stadthistorische Bibliothek Bonn",
        "prediction": "False",
        "true answers": [
          "Stadtarchiv und Stadthistorische Bibliothek Bonn"
        ]
      },
      {
        "question index": 367,
        "answer": "Sun Yat-sen Mausoleum",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 374,
        "answer": "Belgium",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 375,
        "answer": "Mercury-Redstone 3",
        "prediction": "False",
        "true answers": [
          "Mercury-Redstone 3"
        ]
      },
      {
        "question index": 377,
        "answer": "Bouches-de-l'Elbe",
        "prediction": "False",
        "true answers": [
          "Prague",
          "São Paulo"
        ]
      },
      {
        "question index": 381,
        "answer": "Spermaceti",
        "prediction": "True",
        "true answers": [
          "spermaceti"
        ]
      },
      {
        "question index": 388,
        "answer": "International Football Association Board",
        "prediction": "False",
        "true answers": [
          "FIFA"
        ]
      },
      {
        "question index": 410,
        "answer": "Airbus A320",
        "prediction": "True",
        "true answers": [
          "Airbus A320 family"
        ]
      },
      {
        "question index": 419,
        "answer": "Thai AirAsia",
        "prediction": "False",
        "true answers": [
          "AirAsia X",
          "AirAsia Indonesia",
          "Thai AirAsia",
          "AirAsia Philippines",
          "AirAsia Japan",
          "AirAsia India"
        ]
      },
      {
        "question index": 423,
        "answer": "Sierra Madre",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 436,
        "answer": "India",
        "prediction": "True",
        "true answers": [
          "Himachal Pradesh"
        ]
      },
      {
        "question index": 491,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 504,
        "answer": "Syndics of the Drapers' Guild",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 507,
        "answer": "Asia",
        "prediction": "True",
        "true answers": [
          "Asia"
        ]
      },
      {
        "question index": 529,
        "answer": "Westfeste",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 531,
        "answer": "Intouch Holdings",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 579,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      }
    ],
    "accuracy answerable": 0.4166666666666667,
    "accuracy paper answerable": 0.4583333333333333,
    "accuracy with predictions answerable": 0.20833333333333334,
    "accuracy not answerable": 0.13636363636363635,
    "accuracy paper not answerable": 0.22727272727272727,
    "accuracy with predictions not answerable": 0.45454545454545453,
    "final accuracy": 0.2826086956521739,
    "final accuracy paper": 0.34782608695652173,
    "final accuracy with answerable predictions": 0.17391304347826086,
    "final accuracy with not answerable predictions": 0.43478260869565216,
    "final accuracy with all predictions": 0.32608695652173914,
    "final accuracy answerable predictions no I don't know": 0.15555555555555556,
    "final accuracy all predictions no I don't know": 0.3111111111111111
  },
  "string matching type relation contains word": {
    "right answers answerable": 0,
    "not found answerable": 1,
    "wrong answers answerable": 4,
    "right answers paper answerable": 0,
    "wrong answers paper answerable": 5,
    "right prediction wrong answer number answerable": 0,
    "wrong prediction right answer number answerable": 0,
    "total number answerable": 5,
    "operation types statistics": {
      "entity 1 not found": 0,
      "entity 1 wrong answers": 1,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 1,
      "entity 1 right prediction wrong answer number": 1,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 1,
      "relation 1 not found": 0,
      "relation 1 wrong answers": 1,
      "relation 1 right answers paper": 0,
      "relation 1 wrong answers paper": 1,
      "relation 1 right prediction wrong answer number": 1,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 1,
      "entity 2 not found": 1,
      "entity 2 wrong answers": 0,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 1,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 1,
      "entity 2 total number": 1,
      "relation 2 not found": 0,
      "relation 2 wrong answers": 1,
      "relation 2 right answers paper": 0,
      "relation 2 wrong answers paper": 1,
      "relation 2 right prediction wrong answer number": 0,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 1,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 0,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 1,
      "entity 3 right prediction wrong answer number": 0,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 1,
      "relation 3 not found": 0,
      "relation 3 wrong answers": 0,
      "relation 3 right answers paper": 0,
      "relation 3 wrong answers paper": 0,
      "relation 3 right prediction wrong answer number": 0,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 0,
      "accuracy entity 1": 0.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 1.0,
      "accuracy relation 1": 0.0,
      "accuracy paper relation 1": 0.0,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 1.0,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 0.0,
      "accuracy relation 2": 0.0,
      "accuracy paper relation 2": 0.0,
      "accuracy with predictions relation 2": 0.0,
      "accuracy entity 3": 1.0,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 0,
      "accuracy paper relation 3": 0,
      "accuracy with predictions relation 3": 0
    },
    "not found not answerable": 2,
    "wrong answers not answerable": 3,
    "right answers paper not answerable": 0,
    "wrong answers paper not answerable": 5,
    "right prediction wrong answer number not answerable": 2,
    "wrong prediction right answer number not answerable": 1,
    "total number not answerable": 5,
    "total number": 10,
    "answers": [
      {
        "question index": 4,
        "answer": "Australia",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 294,
        "answer": "Italy",
        "prediction": "True",
        "true answers": [
          "American English"
        ]
      },
      {
        "question index": 314,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Manhattan"
        ]
      },
      {
        "question index": 323,
        "answer": "1800-01-01T00:00:00Z",
        "prediction": "False",
        "true answers": [
          "Sengoku period",
          "Nanboku-chō period"
        ]
      },
      {
        "question index": 355,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 361,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 470,
        "answer": "João",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 496,
        "answer": "Australia",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 592,
        "answer": "Australia",
        "prediction": "False",
        "true answers": [
          "Australian Open"
        ]
      },
      {
        "question index": 595,
        "answer": "The Australian",
        "prediction": "False",
        "true answers": [
          "Australia national baseball team",
          "Australia men's national field hockey team",
          "Australia women's national basketball team",
          "Australia national cricket team",
          "Ireland national Australian rules football team",
          "Australia men's national basketball team",
          "Australia Fed Cup team",
          "Australia women's national volleyball team",
          "Australia men's national volleyball team",
          "Australia women's national ice hockey team",
          "Australia women's national field hockey team",
          "New Zealand national Australian rules football team",
          "Norfolk Island national rugby league team",
          "Australia men's national lacrosse team",
          "Australia national indoor lacrosse team",
          "Australia national under-20 rugby union team",
          "Australia women's national softball team",
          "Australia women's national wheelchair basketball team at the 2012 Summer Paralympics",
          "Australia national curling team (mixed)",
          "Australia A cricket team",
          "Australia national speedway team",
          "Australia international rules football team",
          "Australia national netball team",
          "Australia women's international rules football team",
          "Australian Schoolboys national baseball team"
        ]
      }
    ],
    "accuracy answerable": 0.0,
    "accuracy paper answerable": 0.0,
    "accuracy with predictions answerable": 0.0,
    "accuracy not answerable": 0.4,
    "accuracy paper not answerable": 0.0,
    "accuracy with predictions not answerable": 0.6,
    "final accuracy": 0.2,
    "final accuracy paper": 0.0,
    "final accuracy with answerable predictions": 0.2,
    "final accuracy with not answerable predictions": 0.3,
    "final accuracy with all predictions": 0.3,
    "final accuracy answerable predictions no I don't know": 0.2,
    "final accuracy all predictions no I don't know": 0.3
  },
  "count": {
    "right answers answerable": 0,
    "not found answerable": 1,
    "wrong answers answerable": 5,
    "right answers paper answerable": 3,
    "wrong answers paper answerable": 3,
    "right prediction wrong answer number answerable": 1,
    "wrong prediction right answer number answerable": 0,
    "total number answerable": 6,
    "operation types statistics": {
      "entity 1 not found": 1,
      "entity 1 wrong answers": 0,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 1,
      "entity 1 right prediction wrong answer number": 0,
      "entity 1 wrong prediction right answer number": 1,
      "entity 1 total number": 1,
      "relation 1 not found": 0,
      "relation 1 wrong answers": 1,
      "relation 1 right answers paper": 1,
      "relation 1 wrong answers paper": 0,
      "relation 1 right prediction wrong answer number": 0,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 1,
      "entity 2 not found": 0,
      "entity 2 wrong answers": 1,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 1,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 1,
      "relation 2 not found": 1,
      "relation 2 wrong answers": 0,
      "relation 2 right answers paper": 1,
      "relation 2 wrong answers paper": 0,
      "relation 2 right prediction wrong answer number": 0,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 1,
      "entity 3 not found": 0,
      "entity 3 wrong answers": 0,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 0,
      "entity 3 right prediction wrong answer number": 0,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 0,
      "relation 3 not found": 0,
      "relation 3 wrong answers": 1,
      "relation 3 right answers paper": 1,
      "relation 3 wrong answers paper": 0,
      "relation 3 right prediction wrong answer number": 1,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 1,
      "accuracy entity 1": 1.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 0.0,
      "accuracy relation 1": 0.0,
      "accuracy paper relation 1": 1.0,
      "accuracy with predictions relation 1": 0.0,
      "accuracy entity 2": 0.0,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 0.0,
      "accuracy relation 2": 1.0,
      "accuracy paper relation 2": 1.0,
      "accuracy with predictions relation 2": 1.0,
      "accuracy entity 3": 0,
      "accuracy paper entity 3": 0,
      "accuracy with predictions entity 3": 0,
      "accuracy relation 3": 0.0,
      "accuracy paper relation 3": 1.0,
      "accuracy with predictions relation 3": 1.0
    },
    "not found not answerable": 2,
    "wrong answers not answerable": 3,
    "right answers paper not answerable": 3,
    "wrong answers paper not answerable": 2,
    "right prediction wrong answer number not answerable": 1,
    "wrong prediction right answer number not answerable": 1,
    "total number not answerable": 5,
    "total number": 11,
    "answers": [
      {
        "question index": 5,
        "answer": "1",
        "prediction": "True",
        "true answers": [
          "6"
        ]
      },
      {
        "question index": 27,
        "answer": "2",
        "prediction": "False",
        "true answers": [
          "27"
        ]
      },
      {
        "question index": 135,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 318,
        "answer": "1",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 370,
        "answer": "1",
        "prediction": "False",
        "true answers": [
          "6"
        ]
      },
      {
        "question index": 481,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "4"
        ]
      },
      {
        "question index": 502,
        "answer": "1",
        "prediction": "True",
        "true answers": [
          "9"
        ]
      },
      {
        "question index": 559,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 582,
        "answer": "nominative case",
        "prediction": "False",
        "true answers": [
          "10"
        ]
      },
      {
        "question index": 583,
        "answer": "33",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 655,
        "answer": "1",
        "prediction": "False",
        "true answers": []
      }
    ],
    "accuracy answerable": 0.0,
    "accuracy paper answerable": 0.5,
    "accuracy with predictions answerable": 0.16666666666666666,
    "accuracy not answerable": 0.4,
    "accuracy paper not answerable": 0.6,
    "accuracy with predictions not answerable": 0.4,
    "final accuracy": 0.18181818181818182,
    "final accuracy paper": 0.5454545454545454,
    "final accuracy with answerable predictions": 0.2727272727272727,
    "final accuracy with not answerable predictions": 0.18181818181818182,
    "final accuracy with all predictions": 0.2727272727272727,
    "final accuracy answerable predictions no I don't know": 0.2,
    "final accuracy all predictions no I don't know": 0.2
  },
  "center 2": {
    "right answers answerable": 8,
    "not found answerable": 21,
    "wrong answers answerable": 19,
    "right answers paper answerable": 37,
    "wrong answers paper answerable": 11,
    "right prediction wrong answer number answerable": 5,
    "wrong prediction right answer number answerable": 4,
    "total number answerable": 48,
    "operation types statistics": {
      "entity 1 not found": 3,
      "entity 1 wrong answers": 5,
      "entity 1 right answers paper": 4,
      "entity 1 wrong answers paper": 4,
      "entity 1 right prediction wrong answer number": 5,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 8,
      "relation 1 not found": 1,
      "relation 1 wrong answers": 7,
      "relation 1 right answers paper": 6,
      "relation 1 wrong answers paper": 2,
      "relation 1 right prediction wrong answer number": 5,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 8,
      "entity 2 not found": 0,
      "entity 2 wrong answers": 6,
      "entity 2 right answers paper": 3,
      "entity 2 wrong answers paper": 3,
      "entity 2 right prediction wrong answer number": 2,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 6,
      "relation 2 not found": 4,
      "relation 2 wrong answers": 3,
      "relation 2 right answers paper": 4,
      "relation 2 wrong answers paper": 3,
      "relation 2 right prediction wrong answer number": 2,
      "relation 2 wrong prediction right answer number": 2,
      "relation 2 total number": 7,
      "entity 3 not found": 0,
      "entity 3 wrong answers": 5,
      "entity 3 right answers paper": 4,
      "entity 3 wrong answers paper": 1,
      "entity 3 right prediction wrong answer number": 5,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 5,
      "relation 3 not found": 3,
      "relation 3 wrong answers": 5,
      "relation 3 right answers paper": 3,
      "relation 3 wrong answers paper": 5,
      "relation 3 right prediction wrong answer number": 1,
      "relation 3 wrong prediction right answer number": 1,
      "relation 3 total number": 8,
      "accuracy entity 1": 0.375,
      "accuracy paper entity 1": 0.5,
      "accuracy with predictions entity 1": 1.0,
      "accuracy relation 1": 0.125,
      "accuracy paper relation 1": 0.75,
      "accuracy with predictions relation 1": 0.75,
      "accuracy entity 2": 0.0,
      "accuracy paper entity 2": 0.5,
      "accuracy with predictions entity 2": 0.3333333333333333,
      "accuracy relation 2": 0.5714285714285714,
      "accuracy paper relation 2": 0.5714285714285714,
      "accuracy with predictions relation 2": 0.5714285714285714,
      "accuracy entity 3": 0.0,
      "accuracy paper entity 3": 0.8,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 0.375,
      "accuracy paper relation 3": 0.375,
      "accuracy with predictions relation 3": 0.375
    },
    "not found not answerable": 11,
    "wrong answers not answerable": 31,
    "right answers paper not answerable": 24,
    "wrong answers paper not answerable": 18,
    "right prediction wrong answer number not answerable": 20,
    "wrong prediction right answer number not answerable": 3,
    "total number not answerable": 42,
    "total number": 90,
    "answers": [
      {
        "question index": 6,
        "answer": "10654",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 15,
        "answer": "1",
        "prediction": "True",
        "true answers": [
          "-3",
          "-2",
          "-1",
          "1",
          "2",
          "3",
          "4",
          "5"
        ]
      },
      {
        "question index": 26,
        "answer": "9919",
        "prediction": "False",
        "true answers": [
          "9919"
        ]
      },
      {
        "question index": 39,
        "answer": "sport shooter",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 42,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "1172"
        ]
      },
      {
        "question index": 66,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 77,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "000014546"
        ]
      },
      {
        "question index": 92,
        "answer": "Alexander the Great",
        "prediction": "True",
        "true answers": [
          "king",
          "pharaoh"
        ]
      },
      {
        "question index": 95,
        "answer": "Hungary",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 96,
        "answer": "center",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 101,
        "answer": "31657",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 106,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "percy-bysshe-shelley"
        ]
      },
      {
        "question index": 121,
        "answer": "Order of Brotherhood and Unity",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 141,
        "answer": "2920",
        "prediction": "False",
        "true answers": [
          "PIM52895"
        ]
      },
      {
        "question index": 143,
        "answer": "100663",
        "prediction": "True",
        "true answers": [
          "10006840"
        ]
      },
      {
        "question index": 147,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "AYO"
        ]
      },
      {
        "question index": 162,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 166,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "8"
        ]
      },
      {
        "question index": 184,
        "answer": "Denkmalgeschütztes Objekt",
        "prediction": "False",
        "true answers": [
          "68612"
        ]
      },
      {
        "question index": 192,
        "answer": "76454650",
        "prediction": "True",
        "true answers": [
          "N-1160-2018"
        ]
      },
      {
        "question index": 195,
        "answer": "Macmillan Publishers",
        "prediction": "False",
        "true answers": [
          "Macmillan Publishers"
        ]
      },
      {
        "question index": 196,
        "answer": "Wiley-Blackwell",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 197,
        "answer": "Frankie Valli",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 198,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 199,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "kollwitz-kaethe"
        ]
      },
      {
        "question index": 205,
        "answer": "1",
        "prediction": "False",
        "true answers": [
          "1"
        ]
      },
      {
        "question index": 216,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "ralf-schumann"
        ]
      },
      {
        "question index": 219,
        "answer": "301651",
        "prediction": "False",
        "true answers": [
          "DA06586737"
        ]
      },
      {
        "question index": 224,
        "answer": "Member of Parliament in the United Kingdom",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 234,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "h1232"
        ]
      },
      {
        "question index": 239,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "RI-51-0000266"
        ]
      },
      {
        "question index": 242,
        "answer": "<math xmlns=http://www.w3.org/1998/Math/MathML display=block>\\n  <semantics>\\n    <mrow class=MJX-TeXAtom-ORD>\\n      <mstyle displaystyle=true scriptlevel=0>\\n        <msup>\\n          <mi>c</mi>\\n          <mrow class=MJX-TeXAtom-ORD>\\n            <mn>2</mn>\\n          </mrow>\\n        </msup>\\n        <mo>=</mo>\\n        <msup>\\n          <mi>a</mi>\\n          <mrow class=MJX-TeXAtom-ORD>\\n            <mn>2</mn>\\n          </mrow>\\n        </msup>\\n        <mo></mo>\\n        <msup>\\n          <mi>b</mi>\\n          <mrow class=MJX-TeXAtom-ORD>\\n            <mn>2</mn>\\n          </mrow>\\n        </msup>\\n      </mstyle>\\n    </mrow>\\n    <annotation encoding=application/x-tex>{\\\\displaystyle c^{2}=a^{2}b^{2}}</annotation>\\n  </semantics>\\n</math>",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 251,
        "answer": "Independence Day",
        "prediction": "False",
        "true answers": [
          "New Year's Day",
          "Europe Day",
          "Easter Monday",
          "Saint George's Day",
          "Victory Day",
          "Dormition of the Theotokos",
          "Mtskhetoba",
          "Independence Day",
          "Baptism of the Lord",
          "Paschal cycle",
          "Bedoba",
          "Day of National Unity of Georgia",
          "Saint Andrew the First-Called Day",
          "Christmas",
          "Easter",
          "International Women's Day",
          "Good Friday",
          "Mother's Day"
        ]
      },
      {
        "question index": 252,
        "answer": "volleyball",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 254,
        "answer": "video game",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 265,
        "answer": "Fikret Amirov",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 266,
        "answer": "267265203",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 267,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "qu"
        ]
      },
      {
        "question index": 275,
        "answer": "31657",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 279,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 295,
        "answer": "22017",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 297,
        "answer": "South Korea",
        "prediction": "False",
        "true answers": [
          "КОР"
        ]
      },
      {
        "question index": 299,
        "answer": "polycaprolactam",
        "prediction": "True",
        "true answers": [
          "50"
        ]
      },
      {
        "question index": 301,
        "answer": "366764",
        "prediction": "False",
        "true answers": [
          "O1176676"
        ]
      },
      {
        "question index": 304,
        "answer": "1716942",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 310,
        "answer": "246609381",
        "prediction": "True",
        "true answers": [
          "246609381",
          "41049769"
        ]
      },
      {
        "question index": 312,
        "answer": "245798",
        "prediction": "False",
        "true answers": [
          "7378"
        ]
      },
      {
        "question index": 325,
        "answer": "axiomatisation",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 335,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 339,
        "answer": "11831",
        "prediction": "False",
        "true answers": [
          "12201"
        ]
      },
      {
        "question index": 350,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "GJ-1",
          "GJ-27"
        ]
      },
      {
        "question index": 358,
        "answer": "painter",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 383,
        "answer": "Syracuse",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 386,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 389,
        "answer": "Lucifer",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 395,
        "answer": "11489",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 397,
        "answer": "Zarela Martinez",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 399,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "1070"
        ]
      },
      {
        "question index": 405,
        "answer": "right triangle",
        "prediction": "True",
        "true answers": [
          "right triangle"
        ]
      },
      {
        "question index": 409,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 416,
        "answer": "1.79",
        "prediction": "True",
        "true answers": [
          "1.79"
        ]
      },
      {
        "question index": 435,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "451533"
        ]
      },
      {
        "question index": 463,
        "answer": "Japanese",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 467,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "offset mushroom cap"
        ]
      },
      {
        "question index": 477,
        "answer": "Ancient Greece",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 483,
        "answer": "Trisakti University",
        "prediction": "False",
        "true answers": [
          "ARS%20HPSI41.jpg",
          "%E1%BB%94i%20%C4%91%C3%A0o%20-pink%20guava.jpg",
          "Ripe%20guava.jpg"
        ]
      },
      {
        "question index": 487,
        "answer": "72982",
        "prediction": "True",
        "true answers": [
          "jamie-gillis"
        ]
      },
      {
        "question index": 494,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "49628"
        ]
      },
      {
        "question index": 497,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "1066"
        ]
      },
      {
        "question index": 509,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 526,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "M5060"
        ]
      },
      {
        "question index": 542,
        "answer": "Valérie Trierweiler",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 548,
        "answer": "82245",
        "prediction": "False",
        "true answers": [
          "13909"
        ]
      },
      {
        "question index": 552,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "95IT3W8JZE"
        ]
      },
      {
        "question index": 557,
        "answer": "Sun",
        "prediction": "True",
        "true answers": [
          "0.47",
          "0.342"
        ]
      },
      {
        "question index": 561,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 562,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "pha303"
        ]
      },
      {
        "question index": 564,
        "answer": "Moscow State University",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 567,
        "answer": "22017",
        "prediction": "False",
        "true answers": [
          "5789"
        ]
      },
      {
        "question index": 571,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 578,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "2261",
          "2260",
          "18609"
        ]
      },
      {
        "question index": 599,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "1355"
        ]
      },
      {
        "question index": 614,
        "answer": "2508593",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 615,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 635,
        "answer": "84231",
        "prediction": "True",
        "true answers": [
          "353855"
        ]
      },
      {
        "question index": 646,
        "answer": "Sphenopalatine artery",
        "prediction": "True",
        "true answers": [
          "sphenopalatine artery"
        ]
      },
      {
        "question index": 657,
        "answer": "Dresden",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 665,
        "answer": "circulatory shock",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 676,
        "answer": "316737285",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 681,
        "answer": "50014",
        "prediction": "True",
        "true answers": [
          "165091"
        ]
      }
    ],
    "accuracy answerable": 0.16666666666666666,
    "accuracy paper answerable": 0.7708333333333334,
    "accuracy with predictions answerable": 0.1875,
    "accuracy not answerable": 0.2619047619047619,
    "accuracy paper not answerable": 0.5714285714285714,
    "accuracy with predictions not answerable": 0.6666666666666666,
    "final accuracy": 0.2111111111111111,
    "final accuracy paper": 0.6777777777777778,
    "final accuracy with answerable predictions": 0.2222222222222222,
    "final accuracy with not answerable predictions": 0.4,
    "final accuracy with all predictions": 0.4111111111111111,
    "final accuracy answerable predictions no I don't know": 0.17647058823529413,
    "final accuracy all predictions no I don't know": 0.3764705882352941
  },
  "right subgraph 2": {
    "right answers answerable": 1,
    "not found answerable": 3,
    "wrong answers answerable": 36,
    "right answers paper answerable": 30,
    "wrong answers paper answerable": 10,
    "right prediction wrong answer number answerable": 2,
    "wrong prediction right answer number answerable": 0,
    "total number answerable": 40,
    "operation types statistics": {
      "entity 1 not found": 1,
      "entity 1 wrong answers": 8,
      "entity 1 right answers paper": 4,
      "entity 1 wrong answers paper": 5,
      "entity 1 right prediction wrong answer number": 3,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 9,
      "relation 1 not found": 1,
      "relation 1 wrong answers": 4,
      "relation 1 right answers paper": 3,
      "relation 1 wrong answers paper": 2,
      "relation 1 right prediction wrong answer number": 4,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 5,
      "entity 2 not found": 0,
      "entity 2 wrong answers": 9,
      "entity 2 right answers paper": 7,
      "entity 2 wrong answers paper": 2,
      "entity 2 right prediction wrong answer number": 5,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 9,
      "relation 2 not found": 0,
      "relation 2 wrong answers": 7,
      "relation 2 right answers paper": 6,
      "relation 2 wrong answers paper": 1,
      "relation 2 right prediction wrong answer number": 5,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 7,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 8,
      "entity 3 right answers paper": 6,
      "entity 3 wrong answers paper": 3,
      "entity 3 right prediction wrong answer number": 4,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 9,
      "relation 3 not found": 0,
      "relation 3 wrong answers": 5,
      "relation 3 right answers paper": 4,
      "relation 3 wrong answers paper": 1,
      "relation 3 right prediction wrong answer number": 3,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 5,
      "accuracy entity 1": 0.1111111111111111,
      "accuracy paper entity 1": 0.4444444444444444,
      "accuracy with predictions entity 1": 0.4444444444444444,
      "accuracy relation 1": 0.2,
      "accuracy paper relation 1": 0.6,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 0.0,
      "accuracy paper entity 2": 0.7777777777777778,
      "accuracy with predictions entity 2": 0.5555555555555556,
      "accuracy relation 2": 0.0,
      "accuracy paper relation 2": 0.8571428571428571,
      "accuracy with predictions relation 2": 0.7142857142857143,
      "accuracy entity 3": 0.1111111111111111,
      "accuracy paper entity 3": 0.6666666666666666,
      "accuracy with predictions entity 3": 0.5555555555555556,
      "accuracy relation 3": 0.0,
      "accuracy paper relation 3": 0.8,
      "accuracy with predictions relation 3": 0.6
    },
    "not found not answerable": 3,
    "wrong answers not answerable": 41,
    "right answers paper not answerable": 30,
    "wrong answers paper not answerable": 14,
    "right prediction wrong answer number not answerable": 24,
    "wrong prediction right answer number not answerable": 0,
    "total number not answerable": 44,
    "total number": 84,
    "answers": [
      {
        "question index": 7,
        "answer": "triathlon",
        "prediction": "True",
        "true answers": [
          "triathlon"
        ]
      },
      {
        "question index": 18,
        "answer": "Republican Party",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 33,
        "answer": "Santa Barbara",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 35,
        "answer": "Aggadah",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 49,
        "answer": "Institut de Génie Informatique et Industriel",
        "prediction": "False",
        "true answers": [
          "aeronautics",
          "artificial satellite",
          "helicopter",
          "avionics",
          "launch vehicle",
          "aerospace manufacturer",
          "business jet",
          "air traffic flow management",
          "air traffic management",
          "aerospace engineering",
          "motor car",
          "rapid transit",
          "railway",
          "logistics",
          "Galileo",
          "High-speed rail",
          "GPS tracking unit",
          "Technical Specifications for Interoperability",
          "supply-chain management",
          "Multimodal transport",
          "intelligent transportation system",
          "GSM-R",
          "European Train Control System",
          "cab signalling",
          "European Rail Traffic Management System",
          "transportation engineering",
          "railway signal",
          "transport network",
          "combined transport",
          "Terrestrial Trunked Radio",
          "rolling stock",
          "Fleet management",
          "passenger information system",
          "TETRAPOL",
          "tram",
          "Transport terrestre",
          "automated guideway transit",
          "electrical engineering",
          "power electronics",
          "automatic control",
          "sustainable development",
          "recycling",
          "product-service system",
          "sediment",
          "waste management",
          "rare earth element",
          "primary raw material",
          "demolition",
          "circular economy"
        ]
      },
      {
        "question index": 72,
        "answer": "Unknown sitter",
        "prediction": "False",
        "true answers": [
          "woman",
          "Rosa",
          "curtain",
          "Rouge",
          "necklace",
          "white people",
          "gown",
          "brown hair"
        ]
      },
      {
        "question index": 82,
        "answer": "fluid",
        "prediction": "True",
        "true answers": [
          "United Nations",
          "African Union",
          "UNESCO",
          "World Health Organization",
          "World Trade Organization",
          "Interpol",
          "Universal Postal Union",
          "Organisation internationale de la Francophonie",
          "International Civil Defence Organisation",
          "World Meteorological Organization",
          "Economic Community of Central African States",
          "International Bank for Reconstruction and Development",
          "International Hydrographic Organization",
          "Organisation of African, Caribbean and Pacific States",
          "African Development Bank",
          "International Telecommunication Union",
          "International Finance Corporation",
          "Southern African Development Community",
          "International Development Association",
          "Organisation for the Prohibition of Chemical Weapons",
          "International Centre for Settlement of Investment Disputes",
          "Multilateral Investment Guarantee Agency",
          "OHADA",
          "Gulf of Guinea Commission"
        ]
      },
      {
        "question index": 86,
        "answer": "116797975",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 91,
        "answer": "poetry",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 104,
        "answer": "Ezzel Dine Zulficar",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 117,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 118,
        "answer": "Gruber Prize for Justice",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 124,
        "answer": "United Kingdom",
        "prediction": "False",
        "true answers": [
          "2--411"
        ]
      },
      {
        "question index": 146,
        "answer": "German",
        "prediction": "False",
        "true answers": [
          "nominative case",
          "dative case",
          "accusative case",
          "genitive case"
        ]
      },
      {
        "question index": 149,
        "answer": "film director",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 163,
        "answer": "Crown of the Kingdom of Poland",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 171,
        "answer": "Indian National Congress",
        "prediction": "False",
        "true answers": [
          "Akbar Road"
        ]
      },
      {
        "question index": 176,
        "answer": "United Kingdom",
        "prediction": "False",
        "true answers": [
          "Boris Johnson"
        ]
      },
      {
        "question index": 177,
        "answer": "Indian National Congress",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 180,
        "answer": "Spanish",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 189,
        "answer": "Galleria Borghese",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 191,
        "answer": "Zaragoza",
        "prediction": "False",
        "true answers": [
          "Utebo",
          "Mediana de Aragón",
          "Tauste",
          "Muel, Zaragoza",
          "La Joyosa",
          "Épila",
          "Zuera",
          "Torres de Berrellén",
          "Cuarte de Huerva",
          "Alagón",
          "Valmadrid",
          "Fuentes de Ebro",
          "Cadrete",
          "Villanueva de Gállego",
          "La Muela",
          "San Mateo de Gállego",
          "Perdiguera",
          "Puebla de Albortón",
          "La Puebla de Alfindén",
          "Bardallur",
          "María de Huerva",
          "Pinseque",
          "Pastriz",
          "El Burgo de Ebro",
          "Bárboles",
          "Castejón de Valdejasa",
          "Sobradiel",
          "Villamayor de Gállego"
        ]
      },
      {
        "question index": 194,
        "answer": "German",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 222,
        "answer": "Galleria Borghese",
        "prediction": "True",
        "true answers": [
          "Borghese Collection"
        ]
      },
      {
        "question index": 227,
        "answer": "set theory",
        "prediction": "False",
        "true answers": [
          "Georg Cantor"
        ]
      },
      {
        "question index": 228,
        "answer": "Garry Trudeau",
        "prediction": "True",
        "true answers": [
          "Doonesbury"
        ]
      },
      {
        "question index": 229,
        "answer": "The Beatles",
        "prediction": "False",
        "true answers": [
          "Sgt. Pepper's Lonely Hearts Club Band",
          "Let It Be",
          "Revolver",
          "Yesterday",
          "Strawberry Fields Forever",
          "Hey Jude",
          "Ob-La-Di, Ob-La-Da"
        ]
      },
      {
        "question index": 233,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "90185",
          "90174",
          "90091",
          "90099",
          "90189",
          "90001–90068",
          "90070–90084",
          "90086–90089",
          "90093–90097",
          "90101–90103",
          "90291–90293",
          "91040–91043",
          "91303–91308",
          "91342–91349",
          "91352–91353",
          "91356–91357",
          "91364–91367",
          "91401–91499",
          "91601–91609"
        ]
      },
      {
        "question index": 261,
        "answer": "Santa Barbara",
        "prediction": "True",
        "true answers": [
          "Europe"
        ]
      },
      {
        "question index": 269,
        "answer": "The Egoist",
        "prediction": "False",
        "true answers": [
          "H.D.",
          "Harriet Shaw Weaver"
        ]
      },
      {
        "question index": 282,
        "answer": "urban planner",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 289,
        "answer": "Medal For the Defence of the Caucasus",
        "prediction": "False",
        "true answers": [
          "1942-12-22T00:00:00Z",
          "1934-04-16T00:00:00Z",
          "1930-04-06T00:00:00Z",
          "1945-05-09T00:00:00Z",
          "1918-09-16T00:00:00Z",
          "1941-05-16T00:00:00Z",
          "1967-12-26T00:00:00Z",
          "1965-05-07T00:00:00Z",
          "1948-02-22T00:00:00Z",
          "1944-05-01T00:00:00Z",
          "1944-05-01T00:00:00Z",
          "1945-09-30T00:00:00Z",
          "1942-12-22T00:00:00Z",
          "1938-01-24T00:00:00Z",
          "1969-11-05T00:00:00Z",
          "1957-12-18T00:00:00Z",
          "1957-05-16T00:00:00Z",
          "1939-08-01T00:00:00Z",
          "1947-09-20T00:00:00Z",
          "1913-08-23T00:00:00Z",
          "1970-09-23T00:00:00Z",
          "1970-06-09T00:00:00Z",
          "1967-04-20T00:00:00Z",
          "1926-01-01T00:00:00Z",
          "1913-08-23T00:00:00Z",
          "1913-08-23T00:00:00Z",
          "1913-08-23T00:00:00Z",
          "1913-08-23T00:00:00Z"
        ]
      },
      {
        "question index": 300,
        "answer": "National Inventors Hall of Fame",
        "prediction": "False",
        "true answers": [
          "Albert Einstein Medal",
          "Nobel Prize in Physics",
          "Dannie Heineman Prize for Mathematical Physics",
          "Guggenheim Fellowship",
          "Ernest Orlando Lawrence Award",
          "Helmholtz Medal",
          "Franklin Medal",
          "John J. Carty Award for the Advancement of Science",
          "Richtmyer Memorial Award",
          "William Procter Prize for Scientific Achievement",
          "Foreign Member of the Royal Society",
          "Humanist of the Year",
          "Fellow of the American Physical Society",
          "AAAS Fellow",
          "Fellow of Pakistan Academy of Sciences",
          "Fellow of the Committee for Skeptical Inquiry",
          "Nobel Prize in Physics",
          "Guggenheim Fellowship",
          "Richtmyer Memorial Award",
          "August Wilhelm von Hofmann Medal",
          "Fellow of the American Physical Society",
          "Albert Einstein Medal",
          "Nobel Prize in Physics",
          "National Medal of Science",
          "Bower Award and Prize for Achievement in Science",
          "Guggenheim Fellowship",
          "Josiah Willard Gibbs Lectureship",
          "Oskar Klein Medal",
          "Lars Onsager Prize",
          "Rumford Prize",
          "Benjamin Franklin Medal",
          "AAAS Fellow",
          "Foreign Member of the Royal Society",
          "King Faisal International Prize in Science",
          "Fellow of the American Physical Society",
          "honorary doctor of the Fudan University",
          "honorary doctor of the Chinese University of Hong Kong",
          "Nobel Prize in Physics",
          "Guggenheim Fellowship",
          "Fellow of the American Physical Society",
          "Nobel Prize in Physics",
          "Guggenheim Fellowship",
          "Matteucci Medal",
          "Oskar Klein Medal",
          "Fellow of the American Physical Society",
          "honorary doctor of the Peking University",
          "honorary doctor of the Chinese University of Hong Kong"
        ]
      },
      {
        "question index": 302,
        "answer": "oxcarbazepine",
        "prediction": "False",
        "true answers": [
          "paracetamol"
        ]
      },
      {
        "question index": 322,
        "answer": "United Kingdom",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 341,
        "answer": "Canada",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 343,
        "answer": "Rati Agnihotri",
        "prediction": "False",
        "true answers": [
          "Jennifer Kendal",
          "Sridevi",
          "Yogeeta Bali"
        ]
      },
      {
        "question index": 349,
        "answer": "Kempton Park",
        "prediction": "True",
        "true answers": [
          "Asia",
          "Eurasia"
        ]
      },
      {
        "question index": 351,
        "answer": "Nghệ An",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 364,
        "answer": "Australia",
        "prediction": "False",
        "true answers": [
          "Reserve Bank of Australia"
        ]
      },
      {
        "question index": 379,
        "answer": "1",
        "prediction": "False",
        "true answers": [
          "Eliud Kipchoge",
          "Dennis Kipruto Kimetto",
          "Kenenisa Bekele Beyecha"
        ]
      },
      {
        "question index": 380,
        "answer": "1877-05-24T00:00:00Z",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 391,
        "answer": "Russian Empire",
        "prediction": "True",
        "true answers": [
          "nominative case",
          "accusative case",
          "genitive case",
          "essive case",
          "ablative case",
          "adessive case",
          "inessive case",
          "abessive case",
          "elative case",
          "illative case",
          "distributive case",
          "allative case",
          "comitative case",
          "partitive case",
          "translative case",
          "instructive case",
          "temporal case",
          "nominative case",
          "dative case",
          "accusative case",
          "genitive case",
          "vocative case",
          "instrumental case",
          "locative case",
          "partitive case",
          "prepositional case",
          "nominative case",
          "genitive case"
        ]
      },
      {
        "question index": 404,
        "answer": "Concours Centrale-Supélec",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 407,
        "answer": "Atlanta",
        "prediction": "False",
        "true answers": [
          "Muriel Bowser",
          "Keisha Lance Bottoms"
        ]
      },
      {
        "question index": 411,
        "answer": "19168",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 413,
        "answer": "Silbernes Lorbeerblatt",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 434,
        "answer": "William Jones",
        "prediction": "False",
        "true answers": [
          "English"
        ]
      },
      {
        "question index": 439,
        "answer": "The Egoist",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 442,
        "answer": "United Kingdom",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 446,
        "answer": "Belgium",
        "prediction": "True",
        "true answers": [
          "United Nations",
          "NATO",
          "Commonwealth of Nations",
          "UNESCO",
          "World Health Organization",
          "World Trade Organization",
          "Interpol",
          "Universal Postal Union",
          "G20",
          "G8",
          "League of Nations",
          "Organisation for Economic Cooperation and Development",
          "Organization for Security and Co-operation in Europe",
          "Organization of American States",
          "Organisation internationale de la Francophonie",
          "World Meteorological Organization",
          "Asia-Pacific Economic Cooperation",
          "North American Free Trade Agreement",
          "Asian Development Bank",
          "International Bank for Reconstruction and Development",
          "International Hydrographic Organization",
          "African Development Bank",
          "International Telecommunication Union",
          "International Finance Corporation",
          "Arctic Council",
          "Australia Group",
          "International Energy Agency",
          "International Development Association",
          "Organisation for the Prohibition of Chemical Weapons",
          "International Centre for Settlement of Investment Disputes",
          "Multilateral Investment Guarantee Agency",
          "Missile Technology Control Regime",
          "Nuclear Suppliers Group",
          "Treaty on Open Skies",
          "ABCA Armies",
          "Caribbean Development Bank",
          "Movement Coordination Centre Europe",
          "AUSCANNZUKUS",
          "Air and Space Interoperability Council",
          "Combined Communications Electronics Board",
          "Group on Earth Observations",
          "The Technical Cooperation Program",
          "Association of Southeast Asian Nations Regional Forum",
          "Five Eyes",
          "European Union",
          "United Nations",
          "NATO",
          "UNESCO",
          "World Health Organization",
          "World Trade Organization"
        ]
      },
      {
        "question index": 455,
        "answer": "white",
        "prediction": "True",
        "true answers": [
          "FFFFFF"
        ]
      },
      {
        "question index": 459,
        "answer": "Germany",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 465,
        "answer": "oxcarbazepine",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 484,
        "answer": "triathlon",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 492,
        "answer": "white",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 508,
        "answer": "Germano Celant",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 520,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 524,
        "answer": "Haouza",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 537,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "masculine",
          "feminine",
          "masculine",
          "feminine",
          "neuter",
          "masculine",
          "common gender",
          "feminine",
          "neuter"
        ]
      },
      {
        "question index": 543,
        "answer": "Greece",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 551,
        "answer": "H. G. Wells",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 568,
        "answer": "basketball player",
        "prediction": "False",
        "true answers": [
          "NASA",
          "University of Arizona"
        ]
      },
      {
        "question index": 573,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Vedic Sanskrit"
        ]
      },
      {
        "question index": 575,
        "answer": "geographic region",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 576,
        "answer": "television actor",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 581,
        "answer": "Academy Award for Best Story",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 587,
        "answer": "United States of America",
        "prediction": "True",
        "true answers": [
          "101000000",
          "143000000"
        ]
      },
      {
        "question index": 600,
        "answer": "British Columbia",
        "prediction": "False",
        "true answers": [
          "Point(-123.531 48.299)"
        ]
      },
      {
        "question index": 602,
        "answer": "Lost",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 606,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 617,
        "answer": "FC Voluntari",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 619,
        "answer": "Canada",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 620,
        "answer": "Sierra Entertainment",
        "prediction": "False",
        "true answers": [
          "Roberta Williams",
          "Ken Williams"
        ]
      },
      {
        "question index": 622,
        "answer": "Haouza",
        "prediction": "False",
        "true answers": [
          "Category:People from Alexandria",
          "Category:People from Rostov-on-Don",
          "Category:People from Paderborn",
          "Category:People from Amasya",
          "Category:People from Bolton",
          "Category:People from Volos",
          "Category:People of Suzuka, Mie"
        ]
      },
      {
        "question index": 623,
        "answer": "Pune",
        "prediction": "True",
        "true answers": [
          "aminosalicylic acid",
          "ethambutol",
          "ethionamide",
          "capreomycin",
          "pyrazinamide",
          "cycloserine",
          "isoniazid",
          "Viomycin"
        ]
      },
      {
        "question index": 630,
        "answer": "India",
        "prediction": "False",
        "true answers": [
          "engineer",
          "engineering scientist"
        ]
      },
      {
        "question index": 631,
        "answer": "Unknown sitter",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 633,
        "answer": "Enix",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 636,
        "answer": "Germany",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 639,
        "answer": "William Jones",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 648,
        "answer": "athletics",
        "prediction": "False",
        "true answers": [
          "List of world records in athletics",
          "list of world junior records in athletics",
          "list of South American records in athletics",
          "list of Olympic records in athletics",
          "List of European records in athletics",
          "list of African records in athletics",
          "list of Asian records in athletics",
          "list of North American records in athletics",
          "list of world records in swimming"
        ]
      },
      {
        "question index": 659,
        "answer": "FC Voluntari",
        "prediction": "False",
        "true answers": [
          "Bogdan Zając"
        ]
      },
      {
        "question index": 664,
        "answer": "Charles Jencks",
        "prediction": "True",
        "true answers": [
          "American Academy of Arts and Sciences"
        ]
      }
    ],
    "accuracy answerable": 0.025,
    "accuracy paper answerable": 0.75,
    "accuracy with predictions answerable": 0.075,
    "accuracy not answerable": 0.06818181818181818,
    "accuracy paper not answerable": 0.6818181818181818,
    "accuracy with predictions not answerable": 0.6136363636363636,
    "final accuracy": 0.047619047619047616,
    "final accuracy paper": 0.7142857142857143,
    "final accuracy with answerable predictions": 0.07142857142857142,
    "final accuracy with not answerable predictions": 0.3333333333333333,
    "final accuracy with all predictions": 0.35714285714285715,
    "final accuracy answerable predictions no I don't know": 0.04878048780487805,
    "final accuracy all predictions no I don't know": 0.34146341463414637
  },
  "unknown 2": {
    "right answers answerable": 5,
    "not found answerable": 7,
    "wrong answers answerable": 0,
    "right answers paper answerable": 5,
    "wrong answers paper answerable": 7,
    "right prediction wrong answer number answerable": 5,
    "wrong prediction right answer number answerable": 3,
    "total number answerable": 12,
    "operation types statistics": {
      "entity 1 not found": 2,
      "entity 1 wrong answers": 0,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 2,
      "entity 1 right prediction wrong answer number": 0,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 2,
      "relation 1 not found": 0,
      "relation 1 wrong answers": 0,
      "relation 1 right answers paper": 0,
      "relation 1 wrong answers paper": 0,
      "relation 1 right prediction wrong answer number": 0,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 0,
      "entity 2 not found": 2,
      "entity 2 wrong answers": 0,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 2,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 1,
      "entity 2 total number": 2,
      "relation 2 not found": 2,
      "relation 2 wrong answers": 0,
      "relation 2 right answers paper": 0,
      "relation 2 wrong answers paper": 2,
      "relation 2 right prediction wrong answer number": 0,
      "relation 2 wrong prediction right answer number": 1,
      "relation 2 total number": 2,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 1,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 2,
      "entity 3 right prediction wrong answer number": 1,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 2,
      "relation 3 not found": 1,
      "relation 3 wrong answers": 1,
      "relation 3 right answers paper": 1,
      "relation 3 wrong answers paper": 1,
      "relation 3 right prediction wrong answer number": 1,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 2,
      "accuracy entity 1": 1.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 1.0,
      "accuracy relation 1": 0,
      "accuracy paper relation 1": 0,
      "accuracy with predictions relation 1": 0,
      "accuracy entity 2": 1.0,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 0.5,
      "accuracy relation 2": 1.0,
      "accuracy paper relation 2": 0.0,
      "accuracy with predictions relation 2": 0.5,
      "accuracy entity 3": 0.5,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 0.5,
      "accuracy paper relation 3": 0.5,
      "accuracy with predictions relation 3": 1.0
    },
    "not found not answerable": 8,
    "wrong answers not answerable": 2,
    "right answers paper not answerable": 1,
    "wrong answers paper not answerable": 9,
    "right prediction wrong answer number not answerable": 2,
    "wrong prediction right answer number not answerable": 2,
    "total number not answerable": 10,
    "total number": 22,
    "answers": [
      {
        "question index": 9,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 11,
        "answer": "198",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 19,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 21,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Guy III of Spoleto",
          "Charles the Fat",
          "Rudolph II of Burgundy",
          "Louis the Blind",
          "Louis the Blind",
          "Arnulf of Carinthia"
        ]
      },
      {
        "question index": 43,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 56,
        "answer": "bachelor's degree",
        "prediction": "True",
        "true answers": [
          "medicine",
          "bachelor's degree"
        ]
      },
      {
        "question index": 116,
        "answer": "1801-01-01T00:00:00Z",
        "prediction": "True",
        "true answers": [
          "1921-01-01T00:00:00Z",
          "1801-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 119,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "1853-11-15T00:00:00Z",
          "1819-04-04T00:00:00Z"
        ]
      },
      {
        "question index": 249,
        "answer": "5.05",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 298,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 329,
        "answer": "administration",
        "prediction": "False",
        "true answers": [
          "administration",
          "Master of Business Administration"
        ]
      },
      {
        "question index": 337,
        "answer": "5.05",
        "prediction": "False",
        "true answers": [
          "5.05",
          "Beijing",
          "5.06",
          "Zürich",
          "5.06",
          "Zürich"
        ]
      },
      {
        "question index": 373,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 447,
        "answer": "0",
        "prediction": "False",
        "true answers": [
          "0",
          "2002-01-01T00:00:00Z",
          "200000000000000000000000000",
          "1991-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 515,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 523,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Louis II of Italy",
          "Louis the Pious",
          "Lothair II of Lotharingia",
          "Louis the Pious",
          "Charles of Provence",
          "Louis the Pious"
        ]
      },
      {
        "question index": 553,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "6.04",
          "Eugene"
        ]
      },
      {
        "question index": 574,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 640,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 662,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "dissolution of parliament",
          "1951 United Kingdom general election"
        ]
      },
      {
        "question index": 667,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Abigail Adams",
          "1765-07-14T00:00:00Z"
        ]
      },
      {
        "question index": 669,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Maria Leopoldina of Austria",
          "1821-03-06T00:00:00Z"
        ]
      }
    ],
    "accuracy answerable": 0.4166666666666667,
    "accuracy paper answerable": 0.4166666666666667,
    "accuracy with predictions answerable": 0.5833333333333334,
    "accuracy not answerable": 0.8,
    "accuracy paper not answerable": 0.1,
    "accuracy with predictions not answerable": 0.8,
    "final accuracy": 0.5909090909090909,
    "final accuracy paper": 0.2727272727272727,
    "final accuracy with answerable predictions": 0.6818181818181818,
    "final accuracy with not answerable predictions": 0.5909090909090909,
    "final accuracy with all predictions": 0.6818181818181818,
    "final accuracy answerable predictions no I don't know": 0.5882352941176471,
    "final accuracy all predictions no I don't know": 0.5882352941176471
  },
  "right subgraph": {
    "right answers answerable": 11,
    "not found answerable": 2,
    "wrong answers answerable": 8,
    "right answers paper answerable": 6,
    "wrong answers paper answerable": 15,
    "right prediction wrong answer number answerable": 0,
    "wrong prediction right answer number answerable": 8,
    "total number answerable": 21,
    "operation types statistics": {
      "entity 1 not found": 0,
      "entity 1 wrong answers": 4,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 4,
      "entity 1 right prediction wrong answer number": 4,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 4,
      "relation 1 not found": 0,
      "relation 1 wrong answers": 2,
      "relation 1 right answers paper": 1,
      "relation 1 wrong answers paper": 1,
      "relation 1 right prediction wrong answer number": 2,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 2,
      "entity 2 not found": 0,
      "entity 2 wrong answers": 3,
      "entity 2 right answers paper": 1,
      "entity 2 wrong answers paper": 2,
      "entity 2 right prediction wrong answer number": 2,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 3,
      "relation 2 not found": 0,
      "relation 2 wrong answers": 4,
      "relation 2 right answers paper": 1,
      "relation 2 wrong answers paper": 3,
      "relation 2 right prediction wrong answer number": 2,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 4,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 3,
      "entity 3 right answers paper": 1,
      "entity 3 wrong answers paper": 3,
      "entity 3 right prediction wrong answer number": 3,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 4,
      "relation 3 not found": 0,
      "relation 3 wrong answers": 4,
      "relation 3 right answers paper": 0,
      "relation 3 wrong answers paper": 4,
      "relation 3 right prediction wrong answer number": 3,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 4,
      "accuracy entity 1": 0.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 1.0,
      "accuracy relation 1": 0.0,
      "accuracy paper relation 1": 0.5,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 0.0,
      "accuracy paper entity 2": 0.3333333333333333,
      "accuracy with predictions entity 2": 0.6666666666666666,
      "accuracy relation 2": 0.0,
      "accuracy paper relation 2": 0.25,
      "accuracy with predictions relation 2": 0.5,
      "accuracy entity 3": 0.25,
      "accuracy paper entity 3": 0.25,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 0.0,
      "accuracy paper relation 3": 0.0,
      "accuracy with predictions relation 3": 0.75
    },
    "not found not answerable": 1,
    "wrong answers not answerable": 20,
    "right answers paper not answerable": 4,
    "wrong answers paper not answerable": 17,
    "right prediction wrong answer number not answerable": 16,
    "wrong prediction right answer number not answerable": 0,
    "total number not answerable": 21,
    "total number": 42,
    "answers": [
      {
        "question index": 13,
        "answer": "city",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 16,
        "answer": "1929-04-26T00:00:00Z",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 20,
        "answer": "UTC+09:00",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 41,
        "answer": "Australia",
        "prediction": "False",
        "true answers": [
          "Edward John Eyre"
        ]
      },
      {
        "question index": 81,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Texas"
        ]
      },
      {
        "question index": 94,
        "answer": "Antarctic Treaty area",
        "prediction": "False",
        "true answers": [
          "Western Australia"
        ]
      },
      {
        "question index": 113,
        "answer": "member of the Chamber of Deputies of Chile",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 114,
        "answer": "Chartres",
        "prediction": "True",
        "true answers": [
          "Chartres"
        ]
      },
      {
        "question index": 129,
        "answer": "composer",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 158,
        "answer": "Order of Saint Benedict",
        "prediction": "True",
        "true answers": [
          "Benedictines"
        ]
      },
      {
        "question index": 169,
        "answer": "transcription, RNA-templated",
        "prediction": "False",
        "true answers": [
          "Audrey Stevens Niyogi"
        ]
      },
      {
        "question index": 187,
        "answer": "Oberlin College",
        "prediction": "False",
        "true answers": [
          "Oberlin College"
        ]
      },
      {
        "question index": 206,
        "answer": "Chartres",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 225,
        "answer": "Borgo Virgilio",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 230,
        "answer": "Texas",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 309,
        "answer": "Saudi Arabia",
        "prediction": "False",
        "true answers": [
          "Saudi Arabia"
        ]
      },
      {
        "question index": 317,
        "answer": "transcription, RNA-templated",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 334,
        "answer": "Juan de Fuca",
        "prediction": "False",
        "true answers": [
          "Juan de Fuca"
        ]
      },
      {
        "question index": 356,
        "answer": "Neubau",
        "prediction": "False",
        "true answers": [
          "Neubau"
        ]
      },
      {
        "question index": 359,
        "answer": "Israel",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 365,
        "answer": "David Bowie",
        "prediction": "False",
        "true answers": [
          "David Bowie"
        ]
      },
      {
        "question index": 368,
        "answer": "Saudi Arabia",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 372,
        "answer": "German",
        "prediction": "False",
        "true answers": [
          "Strafgesetzbuch"
        ]
      },
      {
        "question index": 376,
        "answer": "composer",
        "prediction": "True",
        "true answers": [
          "Ralph Vaughan Williams"
        ]
      },
      {
        "question index": 387,
        "answer": "Rio Grande do Sul",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 394,
        "answer": "India",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 428,
        "answer": "Big Gay Al",
        "prediction": "True",
        "true answers": [
          "Trey Parker",
          "John Lydon",
          "Ozzy Osbourne",
          "Isaac Hayes",
          "Jay Leno",
          "DMX",
          "Mona Marshall",
          "Tim Armstrong",
          "Tomorô Taguchi"
        ]
      },
      {
        "question index": 431,
        "answer": "Australia",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 432,
        "answer": "Israel",
        "prediction": "True",
        "true answers": [
          "Israel"
        ]
      },
      {
        "question index": 460,
        "answer": "Malta",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 501,
        "answer": "Academy Award for Best Actor",
        "prediction": "True",
        "true answers": [
          "Academy Award for Best Actor"
        ]
      },
      {
        "question index": 503,
        "answer": "Rio Grande do Sul",
        "prediction": "False",
        "true answers": [
          "Rio Grande do Sul"
        ]
      },
      {
        "question index": 514,
        "answer": "Order of Saint Benedict",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 517,
        "answer": "Antarctic Treaty area",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 532,
        "answer": "Kazakhstan",
        "prediction": "False",
        "true answers": [
          "Kazakhstan",
          "Uzbekistan"
        ]
      },
      {
        "question index": 550,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Fukushima Prefecture"
        ]
      },
      {
        "question index": 577,
        "answer": "Thulasi Nair",
        "prediction": "True",
        "true answers": [
          "Krishna"
        ]
      },
      {
        "question index": 586,
        "answer": "Angela Bowie",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 598,
        "answer": "Central Committee of the Socialist Unity Party of Germany",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 601,
        "answer": "male",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 634,
        "answer": "Malta",
        "prediction": "False",
        "true answers": [
          "Malta"
        ]
      },
      {
        "question index": 675,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      }
    ],
    "accuracy answerable": 0.5238095238095238,
    "accuracy paper answerable": 0.2857142857142857,
    "accuracy with predictions answerable": 0.14285714285714285,
    "accuracy not answerable": 0.047619047619047616,
    "accuracy paper not answerable": 0.19047619047619047,
    "accuracy with predictions not answerable": 0.8095238095238095,
    "final accuracy": 0.2857142857142857,
    "final accuracy paper": 0.23809523809523808,
    "final accuracy with answerable predictions": 0.09523809523809523,
    "final accuracy with not answerable predictions": 0.6666666666666666,
    "final accuracy with all predictions": 0.47619047619047616,
    "final accuracy answerable predictions no I don't know": 0.09523809523809523,
    "final accuracy all predictions no I don't know": 0.47619047619047616
  },
  "simple question left": {
    "right answers answerable": 7,
    "not found answerable": 5,
    "wrong answers answerable": 11,
    "right answers paper answerable": 5,
    "wrong answers paper answerable": 18,
    "right prediction wrong answer number answerable": 4,
    "wrong prediction right answer number answerable": 2,
    "total number answerable": 23,
    "operation types statistics": {
      "entity 1 not found": 0,
      "entity 1 wrong answers": 4,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 4,
      "entity 1 right prediction wrong answer number": 3,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 4,
      "relation 1 not found": 1,
      "relation 1 wrong answers": 2,
      "relation 1 right answers paper": 1,
      "relation 1 wrong answers paper": 2,
      "relation 1 right prediction wrong answer number": 2,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 3,
      "entity 2 not found": 2,
      "entity 2 wrong answers": 2,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 4,
      "entity 2 right prediction wrong answer number": 1,
      "entity 2 wrong prediction right answer number": 1,
      "entity 2 total number": 4,
      "relation 2 not found": 0,
      "relation 2 wrong answers": 3,
      "relation 2 right answers paper": 0,
      "relation 2 wrong answers paper": 3,
      "relation 2 right prediction wrong answer number": 2,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 3,
      "entity 3 not found": 0,
      "entity 3 wrong answers": 4,
      "entity 3 right answers paper": 2,
      "entity 3 wrong answers paper": 2,
      "entity 3 right prediction wrong answer number": 4,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 4,
      "relation 3 not found": 1,
      "relation 3 wrong answers": 3,
      "relation 3 right answers paper": 2,
      "relation 3 wrong answers paper": 2,
      "relation 3 right prediction wrong answer number": 1,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 4,
      "accuracy entity 1": 0.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 0.75,
      "accuracy relation 1": 0.3333333333333333,
      "accuracy paper relation 1": 0.3333333333333333,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 0.5,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 0.5,
      "accuracy relation 2": 0.0,
      "accuracy paper relation 2": 0.0,
      "accuracy with predictions relation 2": 0.6666666666666666,
      "accuracy entity 3": 0.0,
      "accuracy paper entity 3": 0.5,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 0.25,
      "accuracy paper relation 3": 0.5,
      "accuracy with predictions relation 3": 0.5
    },
    "not found not answerable": 4,
    "wrong answers not answerable": 18,
    "right answers paper not answerable": 5,
    "wrong answers paper not answerable": 17,
    "right prediction wrong answer number not answerable": 13,
    "wrong prediction right answer number not answerable": 1,
    "total number not answerable": 22,
    "total number": 45,
    "answers": [
      {
        "question index": 24,
        "answer": "poet lawyer",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 51,
        "answer": "French",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 78,
        "answer": "Point(20.466666666667 44.816666666667)",
        "prediction": "True",
        "true answers": [
          "Belgrade Nikola Tesla Airport",
          "Lisičji Jarak Airport",
          "Progar Airport"
        ]
      },
      {
        "question index": 84,
        "answer": "Christianity",
        "prediction": "True",
        "true answers": [
          "Jesus"
        ]
      },
      {
        "question index": 89,
        "answer": "Carl-Olof Anderberg",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 108,
        "answer": "Clifton Suspension Bridge",
        "prediction": "False",
        "true answers": [
          "Clifton Suspension Bridge"
        ]
      },
      {
        "question index": 126,
        "answer": "Norway",
        "prediction": "True",
        "true answers": [
          "Norway"
        ]
      },
      {
        "question index": 133,
        "answer": "Kingdom of Hungary",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 137,
        "answer": "Villa Celimontana",
        "prediction": "True",
        "true answers": [
          "Pincian Hill",
          "Parco degli Acquedotti",
          "Horti Clodiae",
          "Villa Gordiani",
          "Parco degli Scipioni",
          "Villa Celimontana",
          "Parco archeologico di Centocelle",
          "Parco di Villa Glori",
          "Parco Torre del Fiscale"
        ]
      },
      {
        "question index": 144,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Austin Powers"
        ]
      },
      {
        "question index": 156,
        "answer": "historical Jesus",
        "prediction": "True",
        "true answers": [
          "historical Jesus"
        ]
      },
      {
        "question index": 179,
        "answer": "television actor",
        "prediction": "False",
        "true answers": [
          "Jak II",
          "Injustice: Gods Among Us",
          "Jak 3",
          "Jak and Daxter: The Lost Frontier",
          "Daxter",
          "Jak X: Combat Racing",
          "The Legend of Spyro: A New Beginning",
          "Shark Tale",
          "Mortal Kombat X",
          "Injustice 2",
          "Mortal Kombat 11",
          "Samurai Jack: Battle Through Time"
        ]
      },
      {
        "question index": 186,
        "answer": "Miyun District",
        "prediction": "False",
        "true answers": [
          "People's Republic of China"
        ]
      },
      {
        "question index": 260,
        "answer": "Bezhin Meadow",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 262,
        "answer": "Rings of Power",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 277,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 280,
        "answer": "Vostok 1",
        "prediction": "True",
        "true answers": [
          "Vostok 1"
        ]
      },
      {
        "question index": 324,
        "answer": "Clifton Suspension Bridge",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 353,
        "answer": "International Academy of Astronautics",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 360,
        "answer": "Indore district",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 392,
        "answer": "ethnoreligious group",
        "prediction": "True",
        "true answers": [
          "Judaism"
        ]
      },
      {
        "question index": 408,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Roderick Jaynes"
        ]
      },
      {
        "question index": 418,
        "answer": "Norway",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 425,
        "answer": "Classic",
        "prediction": "False",
        "true answers": [
          "1930 World Snooker Championship",
          "2012 World Open",
          "2009 Jiangsu Classic",
          "2008 Premier League Snooker",
          "2007 Paul Hunter Classic",
          "1946 World Snooker Championship",
          "Pot Black",
          "World Doubles Championship",
          "2011 Shanghai Masters",
          "1929 World Snooker Championship",
          "2009 Premier League Snooker",
          "2011 Snooker Shoot-Out",
          "1977 UK Championship",
          "1979 UK Championship",
          "1980 UK Championship",
          "2013 German Masters",
          "Players Tour Championship 2011/2012 – Event 7",
          "2011 Wuxi Classic",
          "Scottish Professional Championship",
          "2012 Snooker Shoot-Out",
          "2012 Welsh Open",
          "2011 Premier League Snooker",
          "1952 World Snooker Championship",
          "1938 World Snooker Championship",
          "1940 World Snooker Championship",
          "1948 World Snooker Championship",
          "1949 World Snooker Championship",
          "1950 World Snooker Championship",
          "1951 World Snooker Championship",
          "Premier League Snooker",
          "Euro Players Tour Championship 2010/2011 – Event 6",
          "Strachan Open",
          "2008 Jiangsu Classic",
          "1996 Masters",
          "2009 Masters",
          "2010 Masters",
          "2006 Masters",
          "2010 Premier League Snooker",
          "2012 Wuxi Classic",
          "1985 World Snooker Championship",
          "2010 Power Snooker Masters Trophy",
          "2009 Paul Hunter Classic",
          "1977 World Snooker Championship",
          "1978 World Snooker Championship",
          "Hong Kong Open",
          "1975 Masters",
          "1980 World Snooker Championship",
          "Players Tour Championship 2010/2011 – Finals",
          "1982 World Snooker Championship",
          "1934 World Snooker Championship"
        ]
      },
      {
        "question index": 427,
        "answer": "East Sikkim district",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 429,
        "answer": "Villa Celimontana",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 433,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Don't Fear the Roofer",
          "Descent, Part I"
        ]
      },
      {
        "question index": 445,
        "answer": "Mexico",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 464,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 486,
        "answer": "television actor",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 488,
        "answer": "Romance sentimentale",
        "prediction": "False",
        "true answers": [
          "Romance sentimentale"
        ]
      },
      {
        "question index": 495,
        "answer": "Twelfth Night",
        "prediction": "True",
        "true answers": [
          "Viola"
        ]
      },
      {
        "question index": 512,
        "answer": "The Count of Monte Cristo",
        "prediction": "True",
        "true answers": [
          "The Count of Monte Cristo",
          "Monte Cristo"
        ]
      },
      {
        "question index": 546,
        "answer": "Member of Parliament in the Parliament of England",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 556,
        "answer": "Mexico",
        "prediction": "False",
        "true answers": [
          "Aztec Empire"
        ]
      },
      {
        "question index": 560,
        "answer": "chess",
        "prediction": "False",
        "true answers": [
          "World Chess Championship 2014",
          "World Chess Championship 2018"
        ]
      },
      {
        "question index": 589,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Elizabeth Cromwell"
        ]
      },
      {
        "question index": 603,
        "answer": "Anund",
        "prediction": "True",
        "true answers": [
          "Ikano Bank",
          "INGKA Holding"
        ]
      },
      {
        "question index": 613,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Catherine, Duchess of Cambridge",
          "Prince William, Duke of Cambridge"
        ]
      },
      {
        "question index": 618,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 627,
        "answer": "Hermitage Museum",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 653,
        "answer": "William",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 663,
        "answer": "James",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 678,
        "answer": "Point(75.847222222222 22.720555555556)",
        "prediction": "True",
        "true answers": [
          "Indore Municipal Corporation"
        ]
      },
      {
        "question index": 680,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      }
    ],
    "accuracy answerable": 0.30434782608695654,
    "accuracy paper answerable": 0.21739130434782608,
    "accuracy with predictions answerable": 0.391304347826087,
    "accuracy not answerable": 0.18181818181818182,
    "accuracy paper not answerable": 0.22727272727272727,
    "accuracy with predictions not answerable": 0.7272727272727273,
    "final accuracy": 0.24444444444444444,
    "final accuracy paper": 0.2222222222222222,
    "final accuracy with answerable predictions": 0.28888888888888886,
    "final accuracy with not answerable predictions": 0.5111111111111111,
    "final accuracy with all predictions": 0.5555555555555556,
    "final accuracy answerable predictions no I don't know": 0.21951219512195122,
    "final accuracy all predictions no I don't know": 0.5121951219512195
  },
  "rank 2": {
    "right answers answerable": 0,
    "not found answerable": 6,
    "wrong answers answerable": 0,
    "right answers paper answerable": 0,
    "wrong answers paper answerable": 6,
    "right prediction wrong answer number answerable": 3,
    "wrong prediction right answer number answerable": 0,
    "total number answerable": 6,
    "operation types statistics": {
      "entity 1 not found": 1,
      "entity 1 wrong answers": 0,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 1,
      "entity 1 right prediction wrong answer number": 0,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 1,
      "relation 1 not found": 1,
      "relation 1 wrong answers": 0,
      "relation 1 right answers paper": 0,
      "relation 1 wrong answers paper": 1,
      "relation 1 right prediction wrong answer number": 0,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 1,
      "entity 2 not found": 1,
      "entity 2 wrong answers": 0,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 1,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 1,
      "entity 2 total number": 1,
      "relation 2 not found": 0,
      "relation 2 wrong answers": 0,
      "relation 2 right answers paper": 0,
      "relation 2 wrong answers paper": 0,
      "relation 2 right prediction wrong answer number": 0,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 0,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 0,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 1,
      "entity 3 right prediction wrong answer number": 0,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 1,
      "relation 3 not found": 1,
      "relation 3 wrong answers": 0,
      "relation 3 right answers paper": 0,
      "relation 3 wrong answers paper": 1,
      "relation 3 right prediction wrong answer number": 0,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 1,
      "accuracy entity 1": 1.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 1.0,
      "accuracy relation 1": 1.0,
      "accuracy paper relation 1": 0.0,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 1.0,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 0.0,
      "accuracy relation 2": 0,
      "accuracy paper relation 2": 0,
      "accuracy with predictions relation 2": 0,
      "accuracy entity 3": 1.0,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 1.0,
      "accuracy paper relation 3": 0.0,
      "accuracy with predictions relation 3": 1.0
    },
    "not found not answerable": 5,
    "wrong answers not answerable": 0,
    "right answers paper not answerable": 0,
    "wrong answers paper not answerable": 5,
    "right prediction wrong answer number not answerable": 0,
    "wrong prediction right answer number not answerable": 1,
    "total number not answerable": 5,
    "total number": 11,
    "answers": [
      {
        "question index": 28,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Alphabet Inc."
        ]
      },
      {
        "question index": 37,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 115,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "1959 Mexico hurricane"
        ]
      },
      {
        "question index": 170,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 174,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "astemizole",
          "fluphenazine",
          "iloperidone",
          "risperidone",
          "trifluperidol"
        ]
      },
      {
        "question index": 203,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "German submarine U-47",
          "German submarine U-102",
          "German submarine U-101",
          "German submarine U-100",
          "German submarine U-99"
        ]
      },
      {
        "question index": 366,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Liu Ruopeng"
        ]
      },
      {
        "question index": 456,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 569,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "British Museum place thesaurus"
        ]
      },
      {
        "question index": 607,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 651,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      }
    ],
    "accuracy answerable": 0.0,
    "accuracy paper answerable": 0.0,
    "accuracy with predictions answerable": 0.5,
    "accuracy not answerable": 1.0,
    "accuracy paper not answerable": 0.0,
    "accuracy with predictions not answerable": 0.8,
    "final accuracy": 0.45454545454545453,
    "final accuracy paper": 0.0,
    "final accuracy with answerable predictions": 0.7272727272727273,
    "final accuracy with not answerable predictions": 0.36363636363636365,
    "final accuracy with all predictions": 0.6363636363636364,
    "final accuracy answerable predictions no I don't know": 0.625,
    "final accuracy all predictions no I don't know": 0.5
  },
  "statement property 2": {
    "right answers answerable": 12,
    "not found answerable": 21,
    "wrong answers answerable": 8,
    "right answers paper answerable": 16,
    "wrong answers paper answerable": 25,
    "right prediction wrong answer number answerable": 13,
    "wrong prediction right answer number answerable": 4,
    "total number answerable": 41,
    "operation types statistics": {
      "entity 1 not found": 5,
      "entity 1 wrong answers": 2,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 7,
      "entity 1 right prediction wrong answer number": 2,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 7,
      "relation 1 not found": 4,
      "relation 1 wrong answers": 0,
      "relation 1 right answers paper": 0,
      "relation 1 wrong answers paper": 4,
      "relation 1 right prediction wrong answer number": 0,
      "relation 1 wrong prediction right answer number": 1,
      "relation 1 total number": 4,
      "entity 2 not found": 7,
      "entity 2 wrong answers": 0,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 7,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 2,
      "entity 2 total number": 7,
      "relation 2 not found": 3,
      "relation 2 wrong answers": 3,
      "relation 2 right answers paper": 1,
      "relation 2 wrong answers paper": 5,
      "relation 2 right prediction wrong answer number": 3,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 6,
      "entity 3 not found": 5,
      "entity 3 wrong answers": 2,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 7,
      "entity 3 right prediction wrong answer number": 1,
      "entity 3 wrong prediction right answer number": 3,
      "entity 3 total number": 7,
      "relation 3 not found": 3,
      "relation 3 wrong answers": 4,
      "relation 3 right answers paper": 3,
      "relation 3 wrong answers paper": 4,
      "relation 3 right prediction wrong answer number": 3,
      "relation 3 wrong prediction right answer number": 2,
      "relation 3 total number": 7,
      "accuracy entity 1": 0.7142857142857143,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 1.0,
      "accuracy relation 1": 1.0,
      "accuracy paper relation 1": 0.0,
      "accuracy with predictions relation 1": 0.75,
      "accuracy entity 2": 1.0,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 0.7142857142857143,
      "accuracy relation 2": 0.5,
      "accuracy paper relation 2": 0.16666666666666666,
      "accuracy with predictions relation 2": 1.0,
      "accuracy entity 3": 0.7142857142857143,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 0.42857142857142855,
      "accuracy relation 3": 0.42857142857142855,
      "accuracy paper relation 3": 0.42857142857142855,
      "accuracy with predictions relation 3": 0.5714285714285714
    },
    "not found not answerable": 27,
    "wrong answers not answerable": 11,
    "right answers paper not answerable": 4,
    "wrong answers paper not answerable": 34,
    "right prediction wrong answer number not answerable": 9,
    "wrong prediction right answer number not answerable": 8,
    "total number not answerable": 38,
    "total number": 79,
    "answers": [
      {
        "question index": 46,
        "answer": "1795-01-01T00:00:00Z",
        "prediction": "True",
        "true answers": [
          "1795-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 58,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "District Court of Helsinki"
        ]
      },
      {
        "question index": 68,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "1999-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 87,
        "answer": "Ondi Timoner",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 109,
        "answer": "1965-03-18T00:00:00Z",
        "prediction": "False",
        "true answers": [
          "1965-03-19T00:00:00Z"
        ]
      },
      {
        "question index": 136,
        "answer": "Doctor of Philosophy",
        "prediction": "True",
        "true answers": [
          "Doctor of Philosophy",
          "bachelor's degree"
        ]
      },
      {
        "question index": 139,
        "answer": "1910-01-01T00:00:00Z",
        "prediction": "True",
        "true answers": [
          "1910-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 142,
        "answer": "1689-02-06T00:00:00Z",
        "prediction": "False",
        "true answers": [
          "1"
        ]
      },
      {
        "question index": 148,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "1981-09-21T00:00:00Z"
        ]
      },
      {
        "question index": 157,
        "answer": "2008-01-01T00:00:00Z",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 159,
        "answer": "1978-01-01T00:00:00Z",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 164,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 182,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Point(-2.173 47.2841)"
        ]
      },
      {
        "question index": 200,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "2017-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 202,
        "answer": "Ed Harris",
        "prediction": "True",
        "true answers": [
          "Ed Harris"
        ]
      },
      {
        "question index": 204,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 214,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "liquid"
        ]
      },
      {
        "question index": 221,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 223,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "1892-08-22T00:00:00Z",
          "1908-07-20T00:00:00Z",
          "1911-07-04T00:00:00Z",
          "1905-05-28T00:00:00Z"
        ]
      },
      {
        "question index": 231,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 238,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 250,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 259,
        "answer": "2000-10-18T00:00:00Z",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 268,
        "answer": "Milk",
        "prediction": "True",
        "true answers": [
          "Milk",
          "Big Fish"
        ]
      },
      {
        "question index": 274,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "12289"
        ]
      },
      {
        "question index": 276,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Kreis Schmölln",
          "Kreis Altenburg"
        ]
      },
      {
        "question index": 283,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Ottawa"
        ]
      },
      {
        "question index": 285,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "1991-12-25T00:00:00Z"
        ]
      },
      {
        "question index": 291,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 307,
        "answer": "Moscow",
        "prediction": "True",
        "true answers": [
          "Russian Empire"
        ]
      },
      {
        "question index": 321,
        "answer": "Newfoundland and Labrador",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 333,
        "answer": "1987-11-21T00:00:00Z",
        "prediction": "True",
        "true answers": [
          "1987-11-21T00:00:00Z"
        ]
      },
      {
        "question index": 348,
        "answer": "1974-01-01T00:00:00Z",
        "prediction": "False",
        "true answers": [
          "1974-01-01T00:00:00Z",
          "1972-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 369,
        "answer": "Academy Award for Best Director",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 378,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 398,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "1485-08-31T00:00:00Z"
        ]
      },
      {
        "question index": 406,
        "answer": "Chacabuco Province",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 415,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "1987-01-03T00:00:00Z"
        ]
      },
      {
        "question index": 417,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 426,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 430,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 437,
        "answer": "Thanks of Congress",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 438,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 440,
        "answer": "2016-01-01T00:00:00Z",
        "prediction": "False",
        "true answers": [
          "2016-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 452,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 453,
        "answer": "2008-01-01T00:00:00Z",
        "prediction": "False",
        "true answers": [
          "2008-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 454,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 457,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 458,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "concrete"
        ]
      },
      {
        "question index": 462,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 466,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 472,
        "answer": "China–Japan border",
        "prediction": "True",
        "true answers": [
          "1916-01-01T00:00:00Z",
          "1912-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 482,
        "answer": "Alanis Morissette",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 489,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "BC MS 19c Southey"
        ]
      },
      {
        "question index": 498,
        "answer": "1671-01-01T00:00:00Z",
        "prediction": "False",
        "true answers": [
          "1671-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 510,
        "answer": "Sweden",
        "prediction": "True",
        "true answers": [
          "Sweden",
          "United States of America",
          "United Kingdom",
          "Germany"
        ]
      },
      {
        "question index": 511,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "1986-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 518,
        "answer": "Istituto Nazionale di Statistica",
        "prediction": "False",
        "true answers": [
          "2016-06-30T00:00:00Z"
        ]
      },
      {
        "question index": 534,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 539,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 541,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "1"
        ]
      },
      {
        "question index": 563,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 597,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 604,
        "answer": "Glen Ballard",
        "prediction": "False",
        "true answers": [
          "Unsent",
          "So Pure",
          "Thank U",
          "Underneath",
          "Not as We",
          "In Praise of the Vulnerable Man"
        ]
      },
      {
        "question index": 609,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "2007-01-01T00:00:00Z",
          "1999-01-01T00:00:00Z",
          "2003-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 611,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 625,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "J. R. R. Tolkien"
        ]
      },
      {
        "question index": 628,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 637,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 654,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "1965-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 658,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "3"
        ]
      },
      {
        "question index": 660,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 666,
        "answer": "Stanley Kubrick",
        "prediction": "False",
        "true answers": [
          "Germany",
          "Beverly Hills",
          "United States of America"
        ]
      },
      {
        "question index": 668,
        "answer": "Academy Award for Best Original Musical or Comedy Score",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 671,
        "answer": "1985-01-01T00:00:00Z",
        "prediction": "True",
        "true answers": [
          "1995-01-01T00:00:00Z",
          "1996-01-01T00:00:00Z",
          "1999-01-01T00:00:00Z"
        ]
      },
      {
        "question index": 673,
        "answer": "2016-01-01T00:00:00Z",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 674,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 679,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 682,
        "answer": "Olympic gold medal",
        "prediction": "True",
        "true answers": [
          "Olympic gold medal",
          "Olympic silver medal"
        ]
      }
    ],
    "accuracy answerable": 0.2926829268292683,
    "accuracy paper answerable": 0.3902439024390244,
    "accuracy with predictions answerable": 0.5121951219512195,
    "accuracy not answerable": 0.7105263157894737,
    "accuracy paper not answerable": 0.10526315789473684,
    "accuracy with predictions not answerable": 0.7368421052631579,
    "final accuracy": 0.4936708860759494,
    "final accuracy paper": 0.25316455696202533,
    "final accuracy with answerable predictions": 0.6075949367088608,
    "final accuracy with not answerable predictions": 0.5063291139240507,
    "final accuracy with all predictions": 0.620253164556962,
    "final accuracy answerable predictions no I don't know": 0.5303030303030303,
    "final accuracy all predictions no I don't know": 0.5454545454545454
  },
  "two intentions right subgraph": {
    "right answers answerable": 0,
    "not found answerable": 6,
    "wrong answers answerable": 0,
    "right answers paper answerable": 0,
    "wrong answers paper answerable": 6,
    "right prediction wrong answer number answerable": 3,
    "wrong prediction right answer number answerable": 0,
    "total number answerable": 6,
    "operation types statistics": {
      "entity 1 not found": 1,
      "entity 1 wrong answers": 0,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 1,
      "entity 1 right prediction wrong answer number": 0,
      "entity 1 wrong prediction right answer number": 1,
      "entity 1 total number": 1,
      "relation 1 not found": 1,
      "relation 1 wrong answers": 0,
      "relation 1 right answers paper": 0,
      "relation 1 wrong answers paper": 1,
      "relation 1 right prediction wrong answer number": 0,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 1,
      "entity 2 not found": 1,
      "entity 2 wrong answers": 0,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 1,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 1,
      "relation 2 not found": 1,
      "relation 2 wrong answers": 0,
      "relation 2 right answers paper": 0,
      "relation 2 wrong answers paper": 1,
      "relation 2 right prediction wrong answer number": 0,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 1,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 0,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 1,
      "entity 3 right prediction wrong answer number": 0,
      "entity 3 wrong prediction right answer number": 1,
      "entity 3 total number": 1,
      "relation 3 not found": 1,
      "relation 3 wrong answers": 0,
      "relation 3 right answers paper": 0,
      "relation 3 wrong answers paper": 1,
      "relation 3 right prediction wrong answer number": 0,
      "relation 3 wrong prediction right answer number": 1,
      "relation 3 total number": 1,
      "accuracy entity 1": 1.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 0.0,
      "accuracy relation 1": 1.0,
      "accuracy paper relation 1": 0.0,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 1.0,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 1.0,
      "accuracy relation 2": 1.0,
      "accuracy paper relation 2": 0.0,
      "accuracy with predictions relation 2": 1.0,
      "accuracy entity 3": 1.0,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 0.0,
      "accuracy relation 3": 1.0,
      "accuracy paper relation 3": 0.0,
      "accuracy with predictions relation 3": 0.0
    },
    "not found not answerable": 6,
    "wrong answers not answerable": 0,
    "right answers paper not answerable": 0,
    "wrong answers paper not answerable": 6,
    "right prediction wrong answer number not answerable": 0,
    "wrong prediction right answer number not answerable": 3,
    "total number not answerable": 6,
    "total number": 12,
    "answers": [
      {
        "question index": 53,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 73,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "United States of America",
          "composer",
          "United States of America",
          "record producer",
          "United States of America",
          "singer-songwriter",
          "United States of America",
          "songwriter",
          "United States of America",
          "multi-instrumentalist",
          "United States of America",
          "music video director",
          "United States of America",
          "voice actor",
          "United States of America",
          "choreographer",
          "United States of America",
          "dancer"
        ]
      },
      {
        "question index": 172,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 272,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 278,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "London",
          "Admiral Barrington",
          "East India House",
          "Admiral Barrington",
          "Crosby Hall, London",
          "Admiral Barrington",
          "London",
          "Britannia",
          "East India House",
          "Britannia",
          "Crosby Hall, London",
          "Britannia",
          "London",
          "Earl of Mornington",
          "East India House",
          "Earl of Mornington",
          "Crosby Hall, London",
          "Earl of Mornington",
          "London",
          "HMS Hindostan",
          "East India House",
          "HMS Hindostan",
          "Crosby Hall, London",
          "HMS Hindostan",
          "London",
          "Nemesis",
          "East India House",
          "Nemesis",
          "Crosby Hall, London",
          "Nemesis",
          "London",
          "Punjaub",
          "East India House",
          "Punjaub",
          "Crosby Hall, London",
          "Punjaub",
          "London",
          "Busbridge (1782 EIC ship)",
          "East India House",
          "Busbridge (1782 EIC ship)",
          "Crosby Hall, London",
          "Busbridge (1782 EIC ship)"
        ]
      },
      {
        "question index": 385,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "aviation accident",
          "Monterey"
        ]
      },
      {
        "question index": 400,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": [
          "Zung Self-Rating Anxiety Scale",
          "olanzapine",
          "Beck Anxiety Inventory",
          "olanzapine",
          "Hamilton Anxiety Rating Scale",
          "olanzapine",
          "Karolinska Scales of Personality",
          "olanzapine",
          "Taylor Manifest Anxiety Scale",
          "olanzapine",
          "Zung Self-Rating Anxiety Scale",
          "ziprasidone",
          "Beck Anxiety Inventory",
          "ziprasidone",
          "Hamilton Anxiety Rating Scale",
          "ziprasidone",
          "Karolinska Scales of Personality",
          "ziprasidone",
          "Taylor Manifest Anxiety Scale",
          "ziprasidone",
          "Zung Self-Rating Anxiety Scale",
          "diazepam",
          "Beck Anxiety Inventory",
          "diazepam",
          "Hamilton Anxiety Rating Scale",
          "diazepam",
          "Karolinska Scales of Personality",
          "diazepam",
          "Taylor Manifest Anxiety Scale",
          "diazepam",
          "Zung Self-Rating Anxiety Scale",
          "zolpidem",
          "Beck Anxiety Inventory",
          "zolpidem",
          "Hamilton Anxiety Rating Scale",
          "zolpidem",
          "Karolinska Scales of Personality",
          "zolpidem",
          "Taylor Manifest Anxiety Scale",
          "zolpidem",
          "Zung Self-Rating Anxiety Scale",
          "haloperidol",
          "Beck Anxiety Inventory",
          "haloperidol",
          "Hamilton Anxiety Rating Scale",
          "haloperidol",
          "Karolinska Scales of Personality",
          "haloperidol",
          "Taylor Manifest Anxiety Scale",
          "haloperidol",
          "Zung Self-Rating Anxiety Scale",
          "alprazolam",
          "Beck Anxiety Inventory",
          "alprazolam",
          "Hamilton Anxiety Rating Scale",
          "alprazolam",
          "Karolinska Scales of Personality",
          "alprazolam",
          "Taylor Manifest Anxiety Scale",
          "alprazolam",
          "Zung Self-Rating Anxiety Scale",
          "sertraline",
          "Beck Anxiety Inventory",
          "sertraline",
          "Hamilton Anxiety Rating Scale",
          "sertraline",
          "Karolinska Scales of Personality",
          "sertraline",
          "Taylor Manifest Anxiety Scale",
          "sertraline",
          "Zung Self-Rating Anxiety Scale",
          "clonazepam",
          "Beck Anxiety Inventory",
          "clonazepam",
          "Hamilton Anxiety Rating Scale",
          "clonazepam",
          "Karolinska Scales of Personality",
          "clonazepam",
          "Taylor Manifest Anxiety Scale",
          "clonazepam",
          "Zung Self-Rating Anxiety Scale",
          "lorazepam",
          "Beck Anxiety Inventory",
          "lorazepam",
          "Hamilton Anxiety Rating Scale",
          "lorazepam",
          "Karolinska Scales of Personality",
          "lorazepam",
          "Taylor Manifest Anxiety Scale",
          "lorazepam",
          "Zung Self-Rating Anxiety Scale",
          "paroxetine",
          "Beck Anxiety Inventory",
          "paroxetine",
          "Hamilton Anxiety Rating Scale",
          "paroxetine",
          "Karolinska Scales of Personality",
          "paroxetine",
          "Taylor Manifest Anxiety Scale",
          "paroxetine"
        ]
      },
      {
        "question index": 403,
        "answer": "Not Found",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 555,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 565,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "neurology",
          "LRP1",
          "neurology",
          "TRPM8",
          "neurology",
          "MEF2D",
          "neurology",
          "MMP17",
          "neurology",
          "FHL5",
          "neurology",
          "ASTN2",
          "neurology",
          "MARCHF4",
          "neurology",
          "PRDM16",
          "neurology",
          "PHACTR1"
        ]
      },
      {
        "question index": 570,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Claudia Villafañe",
          "midfielder",
          "Claudia Villafañe",
          "forward"
        ]
      },
      {
        "question index": 677,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      }
    ],
    "accuracy answerable": 0.0,
    "accuracy paper answerable": 0.0,
    "accuracy with predictions answerable": 0.5,
    "accuracy not answerable": 1.0,
    "accuracy paper not answerable": 0.0,
    "accuracy with predictions not answerable": 0.5,
    "final accuracy": 0.5,
    "final accuracy paper": 0.0,
    "final accuracy with answerable predictions": 0.75,
    "final accuracy with not answerable predictions": 0.25,
    "final accuracy with all predictions": 0.5,
    "final accuracy answerable predictions no I don't know": 0.6666666666666666,
    "final accuracy all predictions no I don't know": 0.3333333333333333
  },
  "unknown": {
    "right answers answerable": 1,
    "not found answerable": 4,
    "wrong answers answerable": 1,
    "right answers paper answerable": 2,
    "wrong answers paper answerable": 4,
    "right prediction wrong answer number answerable": 0,
    "wrong prediction right answer number answerable": 1,
    "total number answerable": 6,
    "operation types statistics": {
      "entity 1 not found": 1,
      "entity 1 wrong answers": 0,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 1,
      "entity 1 right prediction wrong answer number": 0,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 1,
      "relation 1 not found": 1,
      "relation 1 wrong answers": 0,
      "relation 1 right answers paper": 0,
      "relation 1 wrong answers paper": 1,
      "relation 1 right prediction wrong answer number": 0,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 1,
      "entity 2 not found": 1,
      "entity 2 wrong answers": 0,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 1,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 1,
      "relation 2 not found": 0,
      "relation 2 wrong answers": 1,
      "relation 2 right answers paper": 0,
      "relation 2 wrong answers paper": 1,
      "relation 2 right prediction wrong answer number": 1,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 1,
      "entity 3 not found": 1,
      "entity 3 wrong answers": 0,
      "entity 3 right answers paper": 0,
      "entity 3 wrong answers paper": 1,
      "entity 3 right prediction wrong answer number": 0,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 1,
      "relation 3 not found": 1,
      "relation 3 wrong answers": 0,
      "relation 3 right answers paper": 0,
      "relation 3 wrong answers paper": 1,
      "relation 3 right prediction wrong answer number": 0,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 1,
      "accuracy entity 1": 1.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 1.0,
      "accuracy relation 1": 1.0,
      "accuracy paper relation 1": 0.0,
      "accuracy with predictions relation 1": 1.0,
      "accuracy entity 2": 1.0,
      "accuracy paper entity 2": 0.0,
      "accuracy with predictions entity 2": 1.0,
      "accuracy relation 2": 0.0,
      "accuracy paper relation 2": 0.0,
      "accuracy with predictions relation 2": 1.0,
      "accuracy entity 3": 1.0,
      "accuracy paper entity 3": 0.0,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 1.0,
      "accuracy paper relation 3": 0.0,
      "accuracy with predictions relation 3": 1.0
    },
    "not found not answerable": 5,
    "wrong answers not answerable": 1,
    "right answers paper not answerable": 0,
    "wrong answers paper not answerable": 6,
    "right prediction wrong answer number not answerable": 1,
    "wrong prediction right answer number not answerable": 0,
    "total number not answerable": 6,
    "total number": 12,
    "answers": [
      {
        "question index": 63,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 175,
        "answer": "Secretary of State for Foreign and Commonwealth Affairs",
        "prediction": "False",
        "true answers": [
          "Secretary of State for Foreign and Commonwealth Affairs",
          "1827-04-20T00:00:00Z",
          "Member of the 4th Parliament of the United Kingdom",
          "1812-09-29T00:00:00Z",
          "President of the Board of Control",
          "1821-01-01T00:00:00Z",
          "ambassador of the United Kingdom to Portugal",
          "1815-01-01T00:00:00Z",
          "Prime Minister of the United Kingdom",
          "1827-08-08T00:00:00Z",
          "Chancellor of the Exchequer",
          "1827-08-08T00:00:00Z",
          "Leader of the House of Commons",
          "1827-04-20T00:00:00Z",
          "Member of the 1st Parliament of the United Kingdom",
          "1802-06-29T00:00:00Z",
          "Member of the 2nd Parliament of the United Kingdom",
          "1806-10-24T00:00:00Z",
          "Member of the 3rd Parliament of the United Kingdom",
          "1807-04-29T00:00:00Z",
          "Member of the 5th Parliament of the United Kingdom",
          "1818-06-10T00:00:00Z",
          "Member of the 6th Parliament of the United Kingdom",
          "1820-02-29T00:00:00Z",
          "Member of the 7th Parliament of the United Kingdom",
          "1822-09-16T00:00:00Z",
          "Member of the 17th Parliament of Great Britain",
          "1796-05-20T00:00:00Z",
          "Member of the 18th Parliament of Great Britain",
          "1801-01-01T00:00:00Z",
          "Member of the 8th Parliament of the United Kingdom",
          "1827-04-10T00:00:00Z",
          "Member of the 8th Parliament of the United Kingdom",
          "1827-08-08T00:00:00Z",
          "Member of the 7th Parliament of the United Kingdom",
          "1826-06-02T00:00:00Z"
        ]
      },
      {
        "question index": 178,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 190,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "75391",
          "Nobel Peace Prize"
        ]
      },
      {
        "question index": 208,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "1910-03-07T00:00:00Z",
          "John Franklin Miller",
          "2009-12-31T00:00:00Z",
          "Greg Nickels",
          "1906-03-05T00:00:00Z",
          "Richard A. Ballinger",
          "1890-07-13T00:00:00Z",
          "Robert Moran",
          "1940-03-05T00:00:00Z",
          "Arthur B. Langlie",
          "1928-03-06T00:00:00Z",
          "Bertha Knight Landes",
          "1960-03-07T00:00:00Z",
          "Gordon Stanley Clinton"
        ]
      },
      {
        "question index": 270,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Yitzhak Rabin",
          "Nobel Peace Prize",
          "Yasser Arafat",
          "Nobel Peace Prize"
        ]
      },
      {
        "question index": 390,
        "answer": "Harold Holt",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 585,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 621,
        "answer": "Cardiff University",
        "prediction": "False",
        "true answers": [
          "English",
          "University of Wales"
        ]
      },
      {
        "question index": 638,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 641,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 644,
        "answer": "Not Found",
        "prediction": "False",
        "true answers": [
          "Fawkner",
          "Member of the Australian House of Representatives",
          "Higgins",
          "Member of the Australian House of Representatives"
        ]
      }
    ],
    "accuracy answerable": 0.16666666666666666,
    "accuracy paper answerable": 0.3333333333333333,
    "accuracy with predictions answerable": 0.0,
    "accuracy not answerable": 0.8333333333333334,
    "accuracy paper not answerable": 0.0,
    "accuracy with predictions not answerable": 1.0,
    "final accuracy": 0.5,
    "final accuracy paper": 0.16666666666666666,
    "final accuracy with answerable predictions": 0.4166666666666667,
    "final accuracy with not answerable predictions": 0.5833333333333334,
    "final accuracy with all predictions": 0.5,
    "final accuracy answerable predictions no I don't know": 0.4166666666666667,
    "final accuracy all predictions no I don't know": 0.5
  },
  "count 2": {
    "right answers answerable": 0,
    "not found answerable": 0,
    "wrong answers answerable": 6,
    "right answers paper answerable": 4,
    "wrong answers paper answerable": 2,
    "right prediction wrong answer number answerable": 0,
    "wrong prediction right answer number answerable": 0,
    "total number answerable": 6,
    "operation types statistics": {
      "entity 1 not found": 0,
      "entity 1 wrong answers": 1,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 1,
      "entity 1 right prediction wrong answer number": 1,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 1,
      "relation 1 not found": 0,
      "relation 1 wrong answers": 0,
      "relation 1 right answers paper": 0,
      "relation 1 wrong answers paper": 0,
      "relation 1 right prediction wrong answer number": 0,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 0,
      "entity 2 not found": 0,
      "entity 2 wrong answers": 1,
      "entity 2 right answers paper": 1,
      "entity 2 wrong answers paper": 0,
      "entity 2 right prediction wrong answer number": 1,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 1,
      "relation 2 not found": 0,
      "relation 2 wrong answers": 1,
      "relation 2 right answers paper": 1,
      "relation 2 wrong answers paper": 0,
      "relation 2 right prediction wrong answer number": 1,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 1,
      "entity 3 not found": 0,
      "entity 3 wrong answers": 1,
      "entity 3 right answers paper": 1,
      "entity 3 wrong answers paper": 0,
      "entity 3 right prediction wrong answer number": 1,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 1,
      "relation 3 not found": 0,
      "relation 3 wrong answers": 1,
      "relation 3 right answers paper": 1,
      "relation 3 wrong answers paper": 0,
      "relation 3 right prediction wrong answer number": 1,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 1,
      "accuracy entity 1": 0.0,
      "accuracy paper entity 1": 0.0,
      "accuracy with predictions entity 1": 1.0,
      "accuracy relation 1": 0,
      "accuracy paper relation 1": 0,
      "accuracy with predictions relation 1": 0,
      "accuracy entity 2": 0.0,
      "accuracy paper entity 2": 1.0,
      "accuracy with predictions entity 2": 1.0,
      "accuracy relation 2": 0.0,
      "accuracy paper relation 2": 1.0,
      "accuracy with predictions relation 2": 1.0,
      "accuracy entity 3": 0.0,
      "accuracy paper entity 3": 1.0,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 0.0,
      "accuracy paper relation 3": 1.0,
      "accuracy with predictions relation 3": 1.0
    },
    "not found not answerable": 0,
    "wrong answers not answerable": 5,
    "right answers paper not answerable": 4,
    "wrong answers paper not answerable": 1,
    "right prediction wrong answer number not answerable": 5,
    "wrong prediction right answer number not answerable": 0,
    "total number not answerable": 5,
    "total number": 11,
    "answers": [
      {
        "question index": 71,
        "answer": "1",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 79,
        "answer": "2",
        "prediction": "False",
        "true answers": [
          "5"
        ]
      },
      {
        "question index": 83,
        "answer": "Monkh Saridag",
        "prediction": "True",
        "true answers": [
          "4"
        ]
      },
      {
        "question index": 125,
        "answer": "Point(171.450805 66.293373)",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 220,
        "answer": "1",
        "prediction": "False",
        "true answers": [
          "119"
        ]
      },
      {
        "question index": 336,
        "answer": "1",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 393,
        "answer": "2",
        "prediction": "True",
        "true answers": [
          "11"
        ]
      },
      {
        "question index": 414,
        "answer": "1",
        "prediction": "True",
        "true answers": [
          "101"
        ]
      },
      {
        "question index": 474,
        "answer": "1",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 566,
        "answer": "1",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 624,
        "answer": "3",
        "prediction": "True",
        "true answers": [
          "13"
        ]
      }
    ],
    "accuracy answerable": 0.0,
    "accuracy paper answerable": 0.6666666666666666,
    "accuracy with predictions answerable": 0.0,
    "accuracy not answerable": 0.0,
    "accuracy paper not answerable": 0.8,
    "accuracy with predictions not answerable": 1.0,
    "final accuracy": 0.0,
    "final accuracy paper": 0.7272727272727273,
    "final accuracy with answerable predictions": 0.0,
    "final accuracy with not answerable predictions": 0.45454545454545453,
    "final accuracy with all predictions": 0.45454545454545453,
    "final accuracy answerable predictions no I don't know": 0.0,
    "final accuracy all predictions no I don't know": 0.45454545454545453
  },
  "center": {
    "right answers answerable": 1,
    "not found answerable": 0,
    "wrong answers answerable": 5,
    "right answers paper answerable": 3,
    "wrong answers paper answerable": 3,
    "right prediction wrong answer number answerable": 0,
    "wrong prediction right answer number answerable": 1,
    "total number answerable": 6,
    "operation types statistics": {
      "entity 1 not found": 0,
      "entity 1 wrong answers": 0,
      "entity 1 right answers paper": 0,
      "entity 1 wrong answers paper": 0,
      "entity 1 right prediction wrong answer number": 0,
      "entity 1 wrong prediction right answer number": 0,
      "entity 1 total number": 0,
      "relation 1 not found": 0,
      "relation 1 wrong answers": 1,
      "relation 1 right answers paper": 1,
      "relation 1 wrong answers paper": 0,
      "relation 1 right prediction wrong answer number": 0,
      "relation 1 wrong prediction right answer number": 0,
      "relation 1 total number": 1,
      "entity 2 not found": 0,
      "entity 2 wrong answers": 0,
      "entity 2 right answers paper": 0,
      "entity 2 wrong answers paper": 0,
      "entity 2 right prediction wrong answer number": 0,
      "entity 2 wrong prediction right answer number": 0,
      "entity 2 total number": 0,
      "relation 2 not found": 0,
      "relation 2 wrong answers": 1,
      "relation 2 right answers paper": 0,
      "relation 2 wrong answers paper": 1,
      "relation 2 right prediction wrong answer number": 1,
      "relation 2 wrong prediction right answer number": 0,
      "relation 2 total number": 1,
      "entity 3 not found": 0,
      "entity 3 wrong answers": 1,
      "entity 3 right answers paper": 1,
      "entity 3 wrong answers paper": 0,
      "entity 3 right prediction wrong answer number": 1,
      "entity 3 wrong prediction right answer number": 0,
      "entity 3 total number": 1,
      "relation 3 not found": 0,
      "relation 3 wrong answers": 1,
      "relation 3 right answers paper": 0,
      "relation 3 wrong answers paper": 1,
      "relation 3 right prediction wrong answer number": 1,
      "relation 3 wrong prediction right answer number": 0,
      "relation 3 total number": 1,
      "accuracy entity 1": 0,
      "accuracy paper entity 1": 0,
      "accuracy with predictions entity 1": 0,
      "accuracy relation 1": 0.0,
      "accuracy paper relation 1": 1.0,
      "accuracy with predictions relation 1": 0.0,
      "accuracy entity 2": 0,
      "accuracy paper entity 2": 0,
      "accuracy with predictions entity 2": 0,
      "accuracy relation 2": 0.0,
      "accuracy paper relation 2": 0.0,
      "accuracy with predictions relation 2": 1.0,
      "accuracy entity 3": 0.0,
      "accuracy paper entity 3": 1.0,
      "accuracy with predictions entity 3": 1.0,
      "accuracy relation 3": 0.0,
      "accuracy paper relation 3": 0.0,
      "accuracy with predictions relation 3": 1.0
    },
    "not found not answerable": 0,
    "wrong answers not answerable": 4,
    "right answers paper not answerable": 2,
    "wrong answers paper not answerable": 2,
    "right prediction wrong answer number not answerable": 3,
    "wrong prediction right answer number not answerable": 0,
    "total number not answerable": 4,
    "total number": 10,
    "answers": [
      {
        "question index": 255,
        "answer": "display server",
        "prediction": "True",
        "true answers": []
      },
      {
        "question index": 287,
        "answer": "painter",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 306,
        "answer": "Unix-like",
        "prediction": "True",
        "true answers": [
          "PekWM"
        ]
      },
      {
        "question index": 311,
        "answer": "cosmetics",
        "prediction": "False",
        "true answers": [
          "oil plant"
        ]
      },
      {
        "question index": 332,
        "answer": "painter",
        "prediction": "True",
        "true answers": [
          "Suillus neoalbidipes",
          "Suillus glandulosipes",
          "Suillus pungens",
          "Suillus pseudobrevipes",
          "Suillus quiescens",
          "Amauroderma rude",
          "Ganoderma sichuanense",
          "Paxillus vernalis",
          "Suillus americanus",
          "Gyromitra ancilis",
          "Astraeus odoratus",
          "Leratiomyces ceres",
          "Astraeus sirindhorniae",
          "Discina ancilis",
          "Bovistella utriformis",
          "Nidulariaceae",
          "Agaricus pilatianus",
          "Hebeloma crustuliniforme",
          "Hypholoma sublateritium",
          "Inocybe erubescens",
          "Calvatia gigantea",
          "Agaricus bisporus",
          "Agaricus campestris",
          "Galerina marginata",
          "Sarcodon imbricatus",
          "Agaricus augustus",
          "Ganoderma lucidum",
          "Agaricus silvicola",
          "Suillus sibiricus",
          "Psilocybe semilanceata",
          "Paxillus involutus",
          "Chalciporus piperatus",
          "Agaricus arvensis",
          "sheathed woodtuft",
          "Hypholoma fasciculare",
          "Agaricus subrufescens",
          "Ganoderma applanatum",
          "Suillus luteus",
          "Lethal webcaps",
          "Agaricus semotus",
          "Astraeus hygrometricus",
          "Boletellus ananas",
          "Lycoperdon echinatum",
          "Suillus granulatus",
          "Suillus mediterraneensis",
          "Suillus bellinii",
          "Agaricus bernardii",
          "Inocybe geophylla",
          "Suillus variegatus",
          "Agaricus xanthodermus"
        ]
      },
      {
        "question index": 340,
        "answer": "film producer",
        "prediction": "False",
        "true answers": []
      },
      {
        "question index": 345,
        "answer": "German presidential election, 1949",
        "prediction": "False",
        "true answers": [
          "Anonymous",
          "On Wings of Fire"
        ]
      },
      {
        "question index": 422,
        "answer": "Leslie Iwerks",
        "prediction": "False",
        "true answers": [
          "Steamboat Willie"
        ]
      },
      {
        "question index": 444,
        "answer": "Winnipeg River",
        "prediction": "False",
        "true answers": [
          "Winnipeg River",
          "Nelson River"
        ]
      },
      {
        "question index": 616,
        "answer": "documentary film",
        "prediction": "False",
        "true answers": []
      }
    ],
    "accuracy answerable": 0.16666666666666666,
    "accuracy paper answerable": 0.5,
    "accuracy with predictions answerable": 0.0,
    "accuracy not answerable": 0.0,
    "accuracy paper not answerable": 0.5,
    "accuracy with predictions not answerable": 0.75,
    "final accuracy": 0.1,
    "final accuracy paper": 0.5,
    "final accuracy with answerable predictions": 0.0,
    "final accuracy with not answerable predictions": 0.4,
    "final accuracy with all predictions": 0.3,
    "final accuracy answerable predictions no I don't know": 0.0,
    "final accuracy all predictions no I don't know": 0.3
  }
}